{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure torch module exists in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "PackageNotInstalledError: Package is not installed in prefix.\r\n",
      "  prefix: /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36\r\n",
      "  package name: conda\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda update conda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.10\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.6.16          |           py36_1         156 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    cudatoolkit: 10.0.130-0                                   \n",
      "    ninja:       1.9.0-py36hfd86e86_0                         \n",
      "    pytorch:     1.1.0-py3.6_cuda10.0.130_cudnn7.5.1_0 pytorch\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:     2019.6.16-py36_0                              --> 2019.6.16-py36_1\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2019.6.16    | 156 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-06 02:06:38--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  23.6MB/s    in 4.5s    \n",
      "\n",
      "2019-08-06 02:06:43 (17.7 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preparing and Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahatma Gandhi, the father of the nation in his quest for India's freedom struggle ignored his own family and son, this movie is about his son Hiralal who feels neglected because of mahatma Gandhi's service to the society. The movie starts off in South Africa where Mahatma Gandhi works as a barrister and fighting the cause of India's freedom against the British. Hirarala arrives in South Africa to help his dad who is a barrister, since gandhi was involved in the freedom struggle, he wanted his wife and children to join in too as a service to the society and as a result hirarlal does not get a chance to complete his education and fails his exams, he gets married to his love gulab (bhoomika Chawla) against his father's wishes. Hiralal has ambitions to travel to england and become a barrister just like his dad but his own dad refuses to grant him a scholarship offered to his family by a businessman and instead gives it away to another person saying that the scholarship should not be limited to his family and should be open to the most deserving student living in colony. This angers hiralal and the rift between father and son increases, hirarlal hates his father for neglecting him and blames him for being uneducated and unemployed. Hiralal struggles to make ends meet and lands up on the streets through failed business attempts and in huge debt, he loses his wife and children. Akshaye Khanna has give a stellar performance as Hiralal gandhi.. all kudos to him, the direction and the script is fantastic, the picturization is excellent. overall an Excellent movie and a must see. I give it a 10/10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What does review_to_words do?\n",
    "\n",
    "The review_to_words method defined above uses BeautifulSoup to remove any html tags that appear and uses the nltk package to tokenize the reviews. As a check to ensure we know how everything is working, try applying review_to_words to one of the reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mahatma',\n",
       " 'gandhi',\n",
       " 'father',\n",
       " 'nation',\n",
       " 'quest',\n",
       " 'india',\n",
       " 'freedom',\n",
       " 'struggl',\n",
       " 'ignor',\n",
       " 'famili',\n",
       " 'son',\n",
       " 'movi',\n",
       " 'son',\n",
       " 'hiral',\n",
       " 'feel',\n",
       " 'neglect',\n",
       " 'mahatma',\n",
       " 'gandhi',\n",
       " 'servic',\n",
       " 'societi',\n",
       " 'movi',\n",
       " 'start',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'mahatma',\n",
       " 'gandhi',\n",
       " 'work',\n",
       " 'barrist',\n",
       " 'fight',\n",
       " 'caus',\n",
       " 'india',\n",
       " 'freedom',\n",
       " 'british',\n",
       " 'hirarala',\n",
       " 'arriv',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'help',\n",
       " 'dad',\n",
       " 'barrist',\n",
       " 'sinc',\n",
       " 'gandhi',\n",
       " 'involv',\n",
       " 'freedom',\n",
       " 'struggl',\n",
       " 'want',\n",
       " 'wife',\n",
       " 'children',\n",
       " 'join',\n",
       " 'servic',\n",
       " 'societi',\n",
       " 'result',\n",
       " 'hirarl',\n",
       " 'get',\n",
       " 'chanc',\n",
       " 'complet',\n",
       " 'educ',\n",
       " 'fail',\n",
       " 'exam',\n",
       " 'get',\n",
       " 'marri',\n",
       " 'love',\n",
       " 'gulab',\n",
       " 'bhoomika',\n",
       " 'chawla',\n",
       " 'father',\n",
       " 'wish',\n",
       " 'hiral',\n",
       " 'ambit',\n",
       " 'travel',\n",
       " 'england',\n",
       " 'becom',\n",
       " 'barrist',\n",
       " 'like',\n",
       " 'dad',\n",
       " 'dad',\n",
       " 'refus',\n",
       " 'grant',\n",
       " 'scholarship',\n",
       " 'offer',\n",
       " 'famili',\n",
       " 'businessman',\n",
       " 'instead',\n",
       " 'give',\n",
       " 'away',\n",
       " 'anoth',\n",
       " 'person',\n",
       " 'say',\n",
       " 'scholarship',\n",
       " 'limit',\n",
       " 'famili',\n",
       " 'open',\n",
       " 'deserv',\n",
       " 'student',\n",
       " 'live',\n",
       " 'coloni',\n",
       " 'anger',\n",
       " 'hiral',\n",
       " 'rift',\n",
       " 'father',\n",
       " 'son',\n",
       " 'increas',\n",
       " 'hirarl',\n",
       " 'hate',\n",
       " 'father',\n",
       " 'neglect',\n",
       " 'blame',\n",
       " 'uneduc',\n",
       " 'unemploy',\n",
       " 'hiral',\n",
       " 'struggl',\n",
       " 'make',\n",
       " 'end',\n",
       " 'meet',\n",
       " 'land',\n",
       " 'street',\n",
       " 'fail',\n",
       " 'busi',\n",
       " 'attempt',\n",
       " 'huge',\n",
       " 'debt',\n",
       " 'lose',\n",
       " 'wife',\n",
       " 'children',\n",
       " 'akshay',\n",
       " 'khanna',\n",
       " 'give',\n",
       " 'stellar',\n",
       " 'perform',\n",
       " 'hiral',\n",
       " 'gandhi',\n",
       " 'kudo',\n",
       " 'direct',\n",
       " 'script',\n",
       " 'fantast',\n",
       " 'pictur',\n",
       " 'excel',\n",
       " 'overal',\n",
       " 'excel',\n",
       " 'movi',\n",
       " 'must',\n",
       " 'see',\n",
       " 'give',\n",
       " '10',\n",
       " '10']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Apply review_to_words to a review (train_X[100] or any other review)\n",
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    print(cache_file)\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            print(\"File not found\")\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        print('Preprocess training and test data to obtain words for each review')\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        print('Preprocess training and test data to obtain words for each review')\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        print('Unpack data loaded from cache file')\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_data.pkl\n",
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "Unpack data loaded from cache file\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    # print(np.concatenate( data[0:5], axis=0 ))\n",
    "    word_counts = Counter(np.concatenate( data, axis=0 ))\n",
    "    # print(word_counts)\n",
    "    # word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # print(sorted_words[-1])\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movi': 2, 'film': 3, 'one': 4, 'like': 5, 'time': 6, 'good': 7, 'make': 8, 'charact': 9, 'get': 10, 'see': 11, 'watch': 12, 'stori': 13, 'even': 14, 'would': 15, 'realli': 16, 'well': 17, 'scene': 18, 'look': 19, 'show': 20, 'much': 21, 'end': 22, 'peopl': 23, 'bad': 24, 'go': 25, 'great': 26, 'also': 27, 'first': 28, 'love': 29, 'think': 30, 'way': 31, 'act': 32, 'play': 33, 'made': 34, 'thing': 35, 'could': 36, 'know': 37, 'say': 38, 'seem': 39, 'work': 40, 'plot': 41, 'two': 42, 'actor': 43, 'year': 44, 'come': 45, 'mani': 46, 'seen': 47, 'take': 48, 'want': 49, 'life': 50, 'never': 51, 'littl': 52, 'best': 53, 'tri': 54, 'man': 55, 'ever': 56, 'give': 57, 'better': 58, 'still': 59, 'perform': 60, 'find': 61, 'feel': 62, 'part': 63, 'back': 64, 'use': 65, 'someth': 66, 'director': 67, 'actual': 68, 'interest': 69, 'lot': 70, 'real': 71, 'old': 72, 'cast': 73, 'though': 74, 'live': 75, 'star': 76, 'enjoy': 77, 'guy': 78, 'anoth': 79, 'new': 80, 'role': 81, 'noth': 82, '10': 83, 'funni': 84, 'music': 85, 'point': 86, 'start': 87, 'set': 88, 'girl': 89, 'origin': 90, 'day': 91, 'world': 92, 'everi': 93, 'believ': 94, 'turn': 95, 'quit': 96, 'us': 97, 'direct': 98, 'thought': 99, 'fact': 100, 'minut': 101, 'horror': 102, 'kill': 103, 'action': 104, 'comedi': 105, 'pretti': 106, 'young': 107, 'wonder': 108, 'happen': 109, 'around': 110, 'got': 111, 'effect': 112, 'right': 113, 'long': 114, 'howev': 115, 'big': 116, 'line': 117, 'famili': 118, 'enough': 119, 'seri': 120, 'may': 121, 'need': 122, 'fan': 123, 'bit': 124, 'script': 125, 'beauti': 126, 'person': 127, 'becom': 128, 'without': 129, 'must': 130, 'alway': 131, 'friend': 132, 'tell': 133, 'reason': 134, 'saw': 135, 'last': 136, 'final': 137, 'kid': 138, 'almost': 139, 'put': 140, 'least': 141, 'sure': 142, 'done': 143, 'whole': 144, 'place': 145, 'complet': 146, 'kind': 147, 'expect': 148, 'differ': 149, 'shot': 150, 'far': 151, 'mean': 152, 'anyth': 153, 'book': 154, 'laugh': 155, 'might': 156, 'name': 157, 'sinc': 158, 'begin': 159, '2': 160, 'probabl': 161, 'woman': 162, 'help': 163, 'entertain': 164, 'let': 165, 'screen': 166, 'call': 167, 'tv': 168, 'moment': 169, 'away': 170, 'read': 171, 'yet': 172, 'rather': 173, 'worst': 174, 'run': 175, 'fun': 176, 'lead': 177, 'hard': 178, 'audienc': 179, 'idea': 180, 'anyon': 181, 'episod': 182, 'american': 183, 'found': 184, 'appear': 185, 'bore': 186, 'especi': 187, 'although': 188, 'hope': 189, 'cours': 190, 'keep': 191, 'anim': 192, 'job': 193, 'goe': 194, 'move': 195, 'sens': 196, 'dvd': 197, 'version': 198, 'war': 199, 'money': 200, 'someon': 201, 'mind': 202, 'mayb': 203, 'problem': 204, 'true': 205, 'hous': 206, 'everyth': 207, 'nice': 208, 'second': 209, 'rate': 210, 'three': 211, 'night': 212, 'follow': 213, 'face': 214, 'recommend': 215, 'main': 216, 'product': 217, 'worth': 218, 'leav': 219, 'human': 220, 'special': 221, 'excel': 222, 'togeth': 223, 'wast': 224, 'sound': 225, 'everyon': 226, 'john': 227, 'hand': 228, '1': 229, 'father': 230, 'later': 231, 'eye': 232, 'said': 233, 'view': 234, 'instead': 235, 'review': 236, 'boy': 237, 'high': 238, 'hour': 239, 'miss': 240, 'talk': 241, 'classic': 242, 'wife': 243, 'understand': 244, 'left': 245, 'care': 246, 'black': 247, 'death': 248, 'open': 249, 'murder': 250, 'write': 251, 'half': 252, 'head': 253, 'rememb': 254, 'chang': 255, 'viewer': 256, 'fight': 257, 'gener': 258, 'surpris': 259, 'includ': 260, 'short': 261, 'die': 262, 'fall': 263, 'less': 264, 'els': 265, 'entir': 266, 'piec': 267, 'involv': 268, 'pictur': 269, 'simpli': 270, 'power': 271, 'home': 272, 'top': 273, 'total': 274, 'usual': 275, 'budget': 276, 'attempt': 277, 'suppos': 278, 'releas': 279, 'hollywood': 280, 'terribl': 281, 'song': 282, 'men': 283, 'possibl': 284, 'featur': 285, 'portray': 286, 'disappoint': 287, 'poor': 288, '3': 289, 'coupl': 290, 'stupid': 291, 'camera': 292, 'dead': 293, 'wrong': 294, 'low': 295, 'produc': 296, 'video': 297, 'either': 298, 'aw': 299, 'definit': 300, 'except': 301, 'rest': 302, 'given': 303, 'absolut': 304, 'women': 305, 'lack': 306, 'word': 307, 'writer': 308, 'titl': 309, 'talent': 310, 'decid': 311, 'full': 312, 'perfect': 313, 'along': 314, 'style': 315, 'close': 316, 'truli': 317, 'school': 318, 'emot': 319, 'save': 320, 'age': 321, 'sex': 322, 'next': 323, 'bring': 324, 'mr': 325, 'case': 326, 'killer': 327, 'heart': 328, 'comment': 329, 'sort': 330, 'creat': 331, 'perhap': 332, 'came': 333, 'brother': 334, 'sever': 335, 'joke': 336, 'art': 337, 'dialogu': 338, 'game': 339, 'small': 340, 'base': 341, 'flick': 342, 'written': 343, 'sequenc': 344, 'meet': 345, 'earli': 346, 'often': 347, 'other': 348, 'mother': 349, 'develop': 350, 'humor': 351, 'actress': 352, 'consid': 353, 'dark': 354, 'guess': 355, 'amaz': 356, 'unfortun': 357, 'light': 358, 'lost': 359, 'exampl': 360, 'cinema': 361, 'drama': 362, 'ye': 363, 'white': 364, 'experi': 365, 'imagin': 366, 'mention': 367, 'stop': 368, 'natur': 369, 'forc': 370, 'manag': 371, 'felt': 372, 'cut': 373, 'present': 374, 'children': 375, 'fail': 376, 'son': 377, 'support': 378, 'car': 379, 'qualiti': 380, 'ask': 381, 'hit': 382, 'side': 383, 'voic': 384, 'extrem': 385, 'impress': 386, 'wors': 387, 'evil': 388, 'stand': 389, 'went': 390, 'certainli': 391, 'basic': 392, 'oh': 393, 'overal': 394, 'favorit': 395, 'horribl': 396, 'mysteri': 397, 'number': 398, 'type': 399, 'danc': 400, 'wait': 401, 'hero': 402, '5': 403, 'alreadi': 404, 'learn': 405, 'matter': 406, '4': 407, 'michael': 408, 'genr': 409, 'fine': 410, 'despit': 411, 'throughout': 412, 'walk': 413, 'success': 414, 'histori': 415, 'question': 416, 'zombi': 417, 'town': 418, 'relationship': 419, 'realiz': 420, 'child': 421, 'past': 422, 'daughter': 423, 'late': 424, 'b': 425, 'wish': 426, 'hate': 427, 'credit': 428, 'event': 429, 'theme': 430, 'touch': 431, 'citi': 432, 'today': 433, 'sometim': 434, 'behind': 435, 'god': 436, 'twist': 437, 'sit': 438, 'annoy': 439, 'deal': 440, 'stay': 441, 'abl': 442, 'rent': 443, 'pleas': 444, 'edit': 445, 'blood': 446, 'deserv': 447, 'anyway': 448, 'comic': 449, 'appar': 450, 'soon': 451, 'gave': 452, 'etc': 453, 'level': 454, 'slow': 455, 'chanc': 456, 'score': 457, 'bodi': 458, 'brilliant': 459, 'incred': 460, 'figur': 461, 'situat': 462, 'major': 463, 'self': 464, 'stuff': 465, 'decent': 466, 'element': 467, 'return': 468, 'dream': 469, 'obvious': 470, 'order': 471, 'continu': 472, 'pace': 473, 'ridicul': 474, 'happi': 475, 'highli': 476, 'add': 477, 'group': 478, 'thank': 479, 'ladi': 480, 'novel': 481, 'speak': 482, 'pain': 483, 'career': 484, 'shoot': 485, 'strang': 486, 'heard': 487, 'sad': 488, 'husband': 489, 'polic': 490, 'import': 491, 'break': 492, 'took': 493, 'strong': 494, 'cannot': 495, 'predict': 496, 'robert': 497, 'violenc': 498, 'hilari': 499, 'recent': 500, 'countri': 501, 'known': 502, 'particularli': 503, 'pick': 504, 'documentari': 505, 'season': 506, 'critic': 507, 'jame': 508, 'compar': 509, 'alon': 510, 'obviou': 511, 'told': 512, 'state': 513, 'visual': 514, 'rock': 515, 'exist': 516, 'offer': 517, 'theater': 518, 'opinion': 519, 'gore': 520, 'crap': 521, 'hold': 522, 'result': 523, 'hear': 524, 'room': 525, 'realiti': 526, 'effort': 527, 'clich': 528, 'thriller': 529, 'caus': 530, 'explain': 531, 'serious': 532, 'sequel': 533, 'king': 534, 'local': 535, 'ago': 536, 'none': 537, 'hell': 538, 'note': 539, 'allow': 540, 'sister': 541, 'david': 542, 'simpl': 543, 'femal': 544, 'deliv': 545, 'ok': 546, 'convinc': 547, 'class': 548, 'check': 549, 'suspens': 550, 'win': 551, 'buy': 552, 'oscar': 553, 'huge': 554, 'valu': 555, 'sexual': 556, 'scari': 557, 'cool': 558, 'excit': 559, 'similar': 560, 'provid': 561, 'apart': 562, 'exactli': 563, 'avoid': 564, 'shown': 565, 'seriou': 566, 'english': 567, 'taken': 568, 'whose': 569, 'cinematographi': 570, 'shock': 571, 'polit': 572, 'spoiler': 573, 'offic': 574, 'across': 575, 'middl': 576, 'pass': 577, 'street': 578, 'messag': 579, 'somewhat': 580, 'charm': 581, 'silli': 582, 'modern': 583, 'confus': 584, 'filmmak': 585, 'form': 586, 'tale': 587, 'singl': 588, 'jack': 589, 'mostli': 590, 'attent': 591, 'carri': 592, 'william': 593, 'sing': 594, 'five': 595, 'subject': 596, 'prove': 597, 'richard': 598, 'stage': 599, 'team': 600, 'unlik': 601, 'cop': 602, 'georg': 603, 'televis': 604, 'monster': 605, 'earth': 606, 'cover': 607, 'villain': 608, 'pay': 609, 'marri': 610, 'toward': 611, 'build': 612, 'parent': 613, 'pull': 614, 'due': 615, 'fill': 616, 'respect': 617, 'dialog': 618, 'four': 619, 'remind': 620, 'futur': 621, 'typic': 622, 'weak': 623, '7': 624, 'cheap': 625, 'intellig': 626, 'british': 627, 'atmospher': 628, '80': 629, 'clearli': 630, 'non': 631, 'dog': 632, 'paul': 633, 'fast': 634, 'knew': 635, 'artist': 636, '8': 637, 'crime': 638, 'easili': 639, 'escap': 640, 'adult': 641, 'doubt': 642, 'detail': 643, 'date': 644, 'member': 645, 'fire': 646, 'romant': 647, 'drive': 648, 'gun': 649, 'straight': 650, 'fit': 651, 'beyond': 652, 'attack': 653, 'imag': 654, 'upon': 655, 'posit': 656, 'whether': 657, 'peter': 658, 'fantast': 659, 'captur': 660, 'appreci': 661, 'aspect': 662, 'ten': 663, 'plan': 664, 'discov': 665, 'remain': 666, 'period': 667, 'near': 668, 'air': 669, 'realist': 670, 'mark': 671, 'red': 672, 'dull': 673, 'adapt': 674, 'within': 675, 'lose': 676, 'spend': 677, 'color': 678, 'materi': 679, 'chase': 680, 'mari': 681, 'storylin': 682, 'forget': 683, 'bunch': 684, 'clear': 685, 'lee': 686, 'victim': 687, 'nearli': 688, 'box': 689, 'york': 690, 'inspir': 691, 'match': 692, 'mess': 693, 'finish': 694, 'standard': 695, 'easi': 696, 'truth': 697, 'suffer': 698, 'busi': 699, 'dramat': 700, 'space': 701, 'bill': 702, 'western': 703, 'e': 704, 'list': 705, 'battl': 706, 'notic': 707, 'de': 708, 'french': 709, 'ad': 710, '9': 711, 'tom': 712, 'larg': 713, 'among': 714, 'eventu': 715, 'accept': 716, 'train': 717, 'agre': 718, 'spirit': 719, 'soundtrack': 720, 'third': 721, 'teenag': 722, 'adventur': 723, 'soldier': 724, 'sorri': 725, 'famou': 726, 'suggest': 727, 'drug': 728, 'normal': 729, 'babi': 730, 'cri': 731, 'troubl': 732, 'ultim': 733, 'contain': 734, 'certain': 735, 'cultur': 736, 'romanc': 737, 'rare': 738, 'lame': 739, 'somehow': 740, 'disney': 741, 'mix': 742, 'gone': 743, 'cartoon': 744, 'student': 745, 'reveal': 746, 'fear': 747, 'kept': 748, 'suck': 749, 'attract': 750, 'appeal': 751, 'premis': 752, 'greatest': 753, 'secret': 754, 'design': 755, 'shame': 756, 'throw': 757, 'scare': 758, 'copi': 759, 'wit': 760, 'admit': 761, 'america': 762, 'particular': 763, 'relat': 764, 'brought': 765, 'screenplay': 766, 'whatev': 767, 'pure': 768, '70': 769, 'averag': 770, 'harri': 771, 'master': 772, 'describ': 773, 'treat': 774, 'male': 775, '20': 776, 'fantasi': 777, 'issu': 778, 'warn': 779, 'inde': 780, 'forward': 781, 'background': 782, 'project': 783, 'free': 784, 'japanes': 785, 'memor': 786, 'poorli': 787, 'award': 788, 'locat': 789, 'potenti': 790, 'amus': 791, 'struggl': 792, 'magic': 793, 'weird': 794, 'societi': 795, 'okay': 796, 'doctor': 797, 'accent': 798, 'imdb': 799, 'water': 800, 'hot': 801, 'express': 802, '30': 803, 'dr': 804, 'alien': 805, 'odd': 806, 'crazi': 807, 'choic': 808, 'studio': 809, 'fiction': 810, 'becam': 811, 'control': 812, 'masterpiec': 813, 'fli': 814, 'difficult': 815, 'joe': 816, 'scream': 817, 'costum': 818, 'lover': 819, 'uniqu': 820, 'refer': 821, 'remak': 822, 'girlfriend': 823, 'vampir': 824, 'prison': 825, 'execut': 826, 'wear': 827, 'jump': 828, 'unless': 829, 'wood': 830, 'creepi': 831, 'cheesi': 832, 'superb': 833, 'otherwis': 834, 'parti': 835, 'ghost': 836, 'roll': 837, 'public': 838, 'mad': 839, 'depict': 840, 'badli': 841, 'jane': 842, 'moral': 843, 'week': 844, 'earlier': 845, 'dumb': 846, 'fi': 847, 'flaw': 848, 'grow': 849, 'deep': 850, 'sci': 851, 'cat': 852, 'maker': 853, 'older': 854, 'connect': 855, 'footag': 856, 'bother': 857, 'plenti': 858, 'outsid': 859, 'stick': 860, 'gay': 861, 'catch': 862, 'co': 863, 'plu': 864, 'popular': 865, 'equal': 866, 'social': 867, 'quickli': 868, 'disturb': 869, 'perfectli': 870, 'dress': 871, '90': 872, 'era': 873, 'mistak': 874, 'lie': 875, 'ride': 876, 'previou': 877, 'combin': 878, 'band': 879, 'concept': 880, 'rich': 881, 'answer': 882, 'surviv': 883, 'front': 884, 'sweet': 885, 'christma': 886, 'insid': 887, 'bare': 888, 'concern': 889, 'eat': 890, 'beat': 891, 'listen': 892, 'ben': 893, 'c': 894, 'serv': 895, 'term': 896, 'la': 897, 'meant': 898, 'german': 899, 'hardli': 900, 'stereotyp': 901, 'law': 902, 'innoc': 903, 'desper': 904, 'memori': 905, 'promis': 906, 'cute': 907, 'intent': 908, 'steal': 909, 'variou': 910, 'inform': 911, 'brain': 912, 'post': 913, 'tone': 914, 'island': 915, 'amount': 916, 'track': 917, 'nuditi': 918, 'compani': 919, 'store': 920, 'claim': 921, '50': 922, 'flat': 923, 'hair': 924, 'land': 925, 'univers': 926, 'danger': 927, 'fairli': 928, 'kick': 929, 'scott': 930, 'player': 931, 'crew': 932, 'step': 933, 'plain': 934, 'toni': 935, 'share': 936, 'tast': 937, 'centuri': 938, 'achiev': 939, 'engag': 940, 'travel': 941, 'cold': 942, 'rip': 943, 'suit': 944, 'record': 945, 'sadli': 946, 'manner': 947, 'wrote': 948, 'tension': 949, 'spot': 950, 'intens': 951, 'fascin': 952, 'familiar': 953, 'remark': 954, 'depth': 955, 'burn': 956, 'histor': 957, 'destroy': 958, 'sleep': 959, 'purpos': 960, 'languag': 961, 'ignor': 962, 'ruin': 963, 'delight': 964, 'unbeliev': 965, 'italian': 966, 'collect': 967, 'soul': 968, 'abil': 969, 'clever': 970, 'detect': 971, 'violent': 972, 'rape': 973, 'reach': 974, 'door': 975, 'liter': 976, 'trash': 977, 'scienc': 978, 'caught': 979, 'commun': 980, 'reveng': 981, 'creatur': 982, 'approach': 983, 'trip': 984, 'intrigu': 985, 'fashion': 986, 'introduc': 987, 'paint': 988, 'skill': 989, 'complex': 990, 'channel': 991, 'camp': 992, 'christian': 993, 'extra': 994, 'hole': 995, 'ann': 996, 'mental': 997, 'limit': 998, 'immedi': 999, '6': 1000, 'million': 1001, 'mere': 1002, 'comput': 1003, 'slightli': 1004, 'conclus': 1005, 'slasher': 1006, 'imposs': 1007, 'suddenli': 1008, 'neither': 1009, 'crimin': 1010, 'teen': 1011, 'physic': 1012, 'nation': 1013, 'spent': 1014, 'respons': 1015, 'planet': 1016, 'fake': 1017, 'receiv': 1018, 'sick': 1019, 'blue': 1020, 'bizarr': 1021, 'embarrass': 1022, 'indian': 1023, 'ring': 1024, '15': 1025, 'pop': 1026, 'drop': 1027, 'drag': 1028, 'haunt': 1029, 'pointless': 1030, 'suspect': 1031, 'edg': 1032, 'search': 1033, 'handl': 1034, 'common': 1035, 'biggest': 1036, 'faith': 1037, 'arriv': 1038, 'hurt': 1039, 'technic': 1040, 'angel': 1041, 'genuin': 1042, 'dad': 1043, 'awesom': 1044, 'solid': 1045, 'f': 1046, 'focu': 1047, 'van': 1048, 'colleg': 1049, 'former': 1050, 'count': 1051, 'heavi': 1052, 'tear': 1053, 'rais': 1054, 'wall': 1055, 'younger': 1056, 'laughabl': 1057, 'visit': 1058, 'excus': 1059, 'fair': 1060, 'sign': 1061, 'cult': 1062, 'tough': 1063, 'motion': 1064, 'key': 1065, 'desir': 1066, 'super': 1067, 'stun': 1068, 'addit': 1069, 'cloth': 1070, 'exploit': 1071, 'smith': 1072, 'tortur': 1073, 'race': 1074, 'davi': 1075, 'cross': 1076, 'author': 1077, 'jim': 1078, 'minor': 1079, 'focus': 1080, 'compel': 1081, 'consist': 1082, 'pathet': 1083, 'chemistri': 1084, 'commit': 1085, 'park': 1086, 'obsess': 1087, 'tradit': 1088, 'frank': 1089, 'grade': 1090, '60': 1091, 'asid': 1092, 'brutal': 1093, 'steve': 1094, 'somewher': 1095, 'opportun': 1096, 'u': 1097, 'grant': 1098, 'rule': 1099, 'explor': 1100, 'depress': 1101, 'besid': 1102, 'honest': 1103, 'dub': 1104, 'anti': 1105, 'trailer': 1106, 'intend': 1107, 'bar': 1108, 'scientist': 1109, 'regard': 1110, 'west': 1111, 'longer': 1112, 'decad': 1113, 'judg': 1114, 'silent': 1115, 'armi': 1116, 'creativ': 1117, 'wild': 1118, 'south': 1119, 'g': 1120, 'stewart': 1121, 'draw': 1122, 'road': 1123, 'govern': 1124, 'ex': 1125, 'boss': 1126, 'practic': 1127, 'gang': 1128, 'festiv': 1129, 'surprisingli': 1130, 'motiv': 1131, 'club': 1132, 'london': 1133, 'redeem': 1134, 'page': 1135, 'green': 1136, 'idiot': 1137, 'machin': 1138, 'display': 1139, 'militari': 1140, 'aliv': 1141, 'thrill': 1142, 'repeat': 1143, 'yeah': 1144, '100': 1145, 'folk': 1146, 'nobodi': 1147, '40': 1148, 'garbag': 1149, 'journey': 1150, 'ground': 1151, 'tire': 1152, 'smile': 1153, 'bought': 1154, 'mood': 1155, 'cost': 1156, 'sam': 1157, 'stone': 1158, 'mouth': 1159, 'noir': 1160, 'terrif': 1161, 'agent': 1162, 'utterli': 1163, 'requir': 1164, 'sexi': 1165, 'area': 1166, 'honestli': 1167, 'report': 1168, 'geniu': 1169, 'humour': 1170, 'glad': 1171, 'investig': 1172, 'enter': 1173, 'serial': 1174, 'occasion': 1175, 'passion': 1176, 'narr': 1177, 'marriag': 1178, 'climax': 1179, 'studi': 1180, 'industri': 1181, 'ship': 1182, 'center': 1183, 'charli': 1184, 'nowher': 1185, 'demon': 1186, 'bear': 1187, 'loos': 1188, 'hors': 1189, 'hang': 1190, 'wow': 1191, 'graphic': 1192, 'admir': 1193, 'giant': 1194, 'send': 1195, 'damn': 1196, 'loud': 1197, 'nake': 1198, 'subtl': 1199, 'rel': 1200, 'profession': 1201, 'blow': 1202, 'bottom': 1203, 'insult': 1204, 'batman': 1205, 'r': 1206, 'doubl': 1207, 'kelli': 1208, 'boyfriend': 1209, 'initi': 1210, 'frame': 1211, 'opera': 1212, 'gem': 1213, 'challeng': 1214, 'drawn': 1215, 'affect': 1216, 'cinemat': 1217, 'church': 1218, 'fulli': 1219, 'evid': 1220, 'seek': 1221, 'l': 1222, 'j': 1223, 'nightmar': 1224, 'arm': 1225, 'conflict': 1226, 'essenti': 1227, 'christoph': 1228, 'wind': 1229, 'henri': 1230, 'grace': 1231, 'narrat': 1232, 'assum': 1233, 'witch': 1234, 'hunt': 1235, 'push': 1236, 'chri': 1237, 'wise': 1238, 'nomin': 1239, 'month': 1240, 'repres': 1241, 'hide': 1242, 'affair': 1243, 'sceneri': 1244, 'avail': 1245, 'justic': 1246, 'thu': 1247, 'bond': 1248, 'smart': 1249, 'outstand': 1250, 'interview': 1251, 'flashback': 1252, 'constantli': 1253, 'presenc': 1254, 'satisfi': 1255, 'bed': 1256, 'central': 1257, 'iron': 1258, 'content': 1259, 'sell': 1260, 'gag': 1261, 'everybodi': 1262, 'slowli': 1263, 'hotel': 1264, 'hire': 1265, 'system': 1266, 'hey': 1267, 'charl': 1268, 'individu': 1269, 'thrown': 1270, 'adam': 1271, 'mediocr': 1272, 'allen': 1273, 'jone': 1274, 'billi': 1275, 'ray': 1276, 'lesson': 1277, 'cameo': 1278, 'photographi': 1279, 'pari': 1280, 'fellow': 1281, 'strike': 1282, 'brief': 1283, 'rise': 1284, 'independ': 1285, 'absurd': 1286, 'neg': 1287, 'impact': 1288, 'phone': 1289, 'ill': 1290, 'born': 1291, 'model': 1292, 'fresh': 1293, 'spoil': 1294, 'angl': 1295, 'abus': 1296, 'likabl': 1297, 'discuss': 1298, 'hill': 1299, 'ahead': 1300, 'sight': 1301, 'photograph': 1302, 'sent': 1303, 'blame': 1304, 'logic': 1305, 'occur': 1306, 'shine': 1307, 'mainli': 1308, 'bruce': 1309, 'skip': 1310, 'commerci': 1311, 'forev': 1312, 'held': 1313, 'segment': 1314, 'surround': 1315, 'teacher': 1316, 'zero': 1317, 'blond': 1318, 'trap': 1319, 'summer': 1320, 'resembl': 1321, 'satir': 1322, 'ball': 1323, 'queen': 1324, 'six': 1325, 'fool': 1326, 'twice': 1327, 'tragedi': 1328, 'sub': 1329, 'reaction': 1330, 'pack': 1331, 'bomb': 1332, 'hospit': 1333, 'will': 1334, 'protagonist': 1335, 'mile': 1336, 'sport': 1337, 'vote': 1338, 'trust': 1339, 'jerri': 1340, 'drink': 1341, 'mom': 1342, 'encount': 1343, 'plane': 1344, 'al': 1345, 'program': 1346, 'station': 1347, 'current': 1348, 'martin': 1349, 'celebr': 1350, 'choos': 1351, 'join': 1352, 'round': 1353, 'tragic': 1354, 'favourit': 1355, 'field': 1356, 'lord': 1357, 'vision': 1358, 'robot': 1359, 'jean': 1360, 'tie': 1361, 'arthur': 1362, 'roger': 1363, 'fortun': 1364, 'random': 1365, 'intern': 1366, 'psycholog': 1367, 'dread': 1368, 'nonsens': 1369, 'prefer': 1370, 'epic': 1371, 'improv': 1372, 'formula': 1373, 'highlight': 1374, 'legend': 1375, 'pleasur': 1376, '11': 1377, 'tape': 1378, 'dollar': 1379, 'thin': 1380, 'gorgeou': 1381, 'wide': 1382, 'object': 1383, 'porn': 1384, 'fox': 1385, 'ugli': 1386, 'buddi': 1387, 'influenc': 1388, 'prepar': 1389, 'ii': 1390, 'nasti': 1391, 'reflect': 1392, 'supposedli': 1393, 'warm': 1394, 'progress': 1395, 'youth': 1396, 'worthi': 1397, 'unusu': 1398, 'length': 1399, 'latter': 1400, 'crash': 1401, 'superior': 1402, 'shop': 1403, 'childhood': 1404, 'seven': 1405, 'remot': 1406, 'theatr': 1407, 'pilot': 1408, 'funniest': 1409, 'disgust': 1410, 'paid': 1411, 'fell': 1412, 'trick': 1413, 'convers': 1414, 'castl': 1415, 'gangster': 1416, 'rob': 1417, 'disast': 1418, 'establish': 1419, 'heaven': 1420, 'disappear': 1421, 'mine': 1422, 'ident': 1423, 'suicid': 1424, 'singer': 1425, 'heroin': 1426, 'forgotten': 1427, 'mask': 1428, 'decis': 1429, 'tend': 1430, 'partner': 1431, 'brian': 1432, 'alan': 1433, 'recogn': 1434, 'desert': 1435, 'thoroughli': 1436, 'stuck': 1437, 'ms': 1438, 'sky': 1439, 'p': 1440, 'replac': 1441, 'accur': 1442, 'market': 1443, 'andi': 1444, 'uncl': 1445, 'seemingli': 1446, 'danni': 1447, 'commentari': 1448, 'eddi': 1449, 'clue': 1450, 'jackson': 1451, 'devil': 1452, 'pair': 1453, 'refus': 1454, 'that': 1455, 'therefor': 1456, 'ed': 1457, 'unit': 1458, 'fate': 1459, 'accid': 1460, 'river': 1461, 'fault': 1462, 'tune': 1463, 'afraid': 1464, 'clean': 1465, 'hidden': 1466, 'russian': 1467, 'stephen': 1468, 'test': 1469, 'captain': 1470, 'irrit': 1471, 'quick': 1472, 'convey': 1473, 'instanc': 1474, 'readi': 1475, 'european': 1476, 'frustrat': 1477, 'insan': 1478, 'daniel': 1479, 'rescu': 1480, '1950': 1481, 'food': 1482, 'chines': 1483, 'wed': 1484, 'dirti': 1485, 'lock': 1486, 'angri': 1487, 'joy': 1488, 'steven': 1489, 'price': 1490, 'bland': 1491, 'cage': 1492, 'rang': 1493, 'anymor': 1494, 'wooden': 1495, 'n': 1496, 'news': 1497, 'rush': 1498, 'jason': 1499, '12': 1500, 'led': 1501, 'board': 1502, 'martial': 1503, 'twenti': 1504, 'worri': 1505, 'hunter': 1506, 'cgi': 1507, 'symbol': 1508, 'transform': 1509, 'invent': 1510, 'onto': 1511, 'sentiment': 1512, 'x': 1513, 'johnni': 1514, 'piti': 1515, 'process': 1516, 'attitud': 1517, 'explan': 1518, 'awar': 1519, 'owner': 1520, 'aim': 1521, 'energi': 1522, 'target': 1523, 'floor': 1524, 'necessari': 1525, 'favor': 1526, 'opposit': 1527, 'religi': 1528, 'blind': 1529, 'insight': 1530, 'window': 1531, 'chick': 1532, 'movement': 1533, 'mountain': 1534, 'deepli': 1535, 'research': 1536, 'comparison': 1537, 'possess': 1538, 'grand': 1539, 'whatsoev': 1540, 'rain': 1541, 'comed': 1542, 'bank': 1543, 'mid': 1544, 'shadow': 1545, 'began': 1546, 'parodi': 1547, 'princ': 1548, 'credibl': 1549, 'taylor': 1550, 'weapon': 1551, 'pre': 1552, 'friendship': 1553, 'flesh': 1554, 'teach': 1555, 'dougla': 1556, 'terror': 1557, 'hint': 1558, 'bloodi': 1559, 'protect': 1560, 'marvel': 1561, 'drunk': 1562, 'anybodi': 1563, 'load': 1564, 'watchabl': 1565, 'superman': 1566, 'leader': 1567, 'accord': 1568, 'brown': 1569, 'freddi': 1570, 'appropri': 1571, 'tim': 1572, 'jeff': 1573, 'seat': 1574, 'hitler': 1575, 'charg': 1576, 'unknown': 1577, 'keaton': 1578, 'villag': 1579, 'knock': 1580, 'enemi': 1581, 'england': 1582, 'media': 1583, 'empti': 1584, 'unnecessari': 1585, 'wave': 1586, 'utter': 1587, 'craft': 1588, 'perspect': 1589, 'buck': 1590, 'dare': 1591, 'strength': 1592, 'ford': 1593, 'nativ': 1594, 'correct': 1595, 'contrast': 1596, 'kiss': 1597, 'speed': 1598, 'anywher': 1599, 'chill': 1600, 'knowledg': 1601, 'nazi': 1602, 'distract': 1603, 'magnific': 1604, 'soap': 1605, 'fred': 1606, 'mission': 1607, 'ice': 1608, '1980': 1609, 'breath': 1610, 'moon': 1611, 'jr': 1612, 'joan': 1613, 'crowd': 1614, 'soft': 1615, '000': 1616, 'frighten': 1617, 'kate': 1618, 'nick': 1619, 'hundr': 1620, 'dick': 1621, 'dan': 1622, 'somebodi': 1623, 'radio': 1624, 'simon': 1625, 'dozen': 1626, 'thousand': 1627, 'academi': 1628, 'andrew': 1629, 'shakespear': 1630, 'loss': 1631, 'sum': 1632, 'root': 1633, 'vehicl': 1634, 'quot': 1635, 'account': 1636, 'behavior': 1637, 'leg': 1638, '1970': 1639, 'convent': 1640, 'gold': 1641, 'regular': 1642, 'pretenti': 1643, 'demand': 1644, 'worker': 1645, 'compet': 1646, 'explos': 1647, 'stretch': 1648, 'lynch': 1649, 'privat': 1650, 'notabl': 1651, 'japan': 1652, 'candi': 1653, 'interpret': 1654, 'debut': 1655, 'constant': 1656, 'tarzan': 1657, 'sea': 1658, 'translat': 1659, 'revolv': 1660, 'spi': 1661, 'prais': 1662, 'quiet': 1663, 'failur': 1664, 'franc': 1665, 'threaten': 1666, 'jesu': 1667, 'technolog': 1668, 'ass': 1669, 'sat': 1670, 'kevin': 1671, 'higher': 1672, 'toy': 1673, 'met': 1674, 'punch': 1675, 'aid': 1676, 'vh': 1677, 'abandon': 1678, 'mike': 1679, 'interact': 1680, 'bet': 1681, 'command': 1682, 'confront': 1683, 'separ': 1684, 'gotten': 1685, 'site': 1686, 'servic': 1687, 'belong': 1688, 'stunt': 1689, 'recal': 1690, 'techniqu': 1691, 'cabl': 1692, 'freak': 1693, 'foot': 1694, 'bug': 1695, 'african': 1696, 'bright': 1697, 'fu': 1698, 'jimmi': 1699, 'capabl': 1700, 'presid': 1701, 'boat': 1702, 'stock': 1703, 'succeed': 1704, 'fat': 1705, 'clark': 1706, 'structur': 1707, 'spanish': 1708, 'gene': 1709, 'paper': 1710, 'kidnap': 1711, 'factor': 1712, 'belief': 1713, 'whilst': 1714, 'tree': 1715, 'educ': 1716, 'complic': 1717, 'attend': 1718, 'realis': 1719, 'witti': 1720, 'realism': 1721, 'bob': 1722, 'broken': 1723, 'finest': 1724, 'santa': 1725, 'assist': 1726, 'observ': 1727, 'up': 1728, 'smoke': 1729, 'v': 1730, 'determin': 1731, 'depart': 1732, 'oper': 1733, 'domin': 1734, 'routin': 1735, 'rubbish': 1736, 'hat': 1737, 'lewi': 1738, 'fame': 1739, 'safe': 1740, 'kinda': 1741, 'hook': 1742, 'foreign': 1743, 'lone': 1744, 'advanc': 1745, 'morgan': 1746, 'rank': 1747, 'numer': 1748, 'vs': 1749, 'shape': 1750, 'shallow': 1751, 'rose': 1752, 'washington': 1753, 'civil': 1754, 'werewolf': 1755, 'morn': 1756, 'gari': 1757, 'accomplish': 1758, 'kong': 1759, 'ordinari': 1760, 'winner': 1761, 'whenev': 1762, 'peac': 1763, 'virtual': 1764, 'grab': 1765, 'luck': 1766, 'h': 1767, 'offens': 1768, 'activ': 1769, 'welcom': 1770, 'bigger': 1771, 'complain': 1772, 'patient': 1773, 'contriv': 1774, 'unfunni': 1775, 'pretend': 1776, 'con': 1777, 'trek': 1778, 'dimension': 1779, 'flash': 1780, 'lesbian': 1781, 'wake': 1782, 'code': 1783, 'dri': 1784, 'cain': 1785, 'eric': 1786, 'statu': 1787, 'albert': 1788, 'guard': 1789, 'corrupt': 1790, 'manipul': 1791, 'dancer': 1792, 'context': 1793, 'sourc': 1794, 'speech': 1795, 'awkward': 1796, 'gain': 1797, 'signific': 1798, 'clip': 1799, 'psycho': 1800, '13': 1801, 'sean': 1802, 'corni': 1803, 'anthoni': 1804, 'reli': 1805, 'religion': 1806, 'w': 1807, 'curiou': 1808, 'theatric': 1809, 'priest': 1810, 'advic': 1811, 'addict': 1812, 'flow': 1813, 'skin': 1814, 'secur': 1815, 'specif': 1816, 'asian': 1817, 'howard': 1818, 'jennif': 1819, 'luke': 1820, 'golden': 1821, 'comfort': 1822, 'organ': 1823, 'core': 1824, 'promot': 1825, 'cheat': 1826, 'lucki': 1827, 'cash': 1828, 'dislik': 1829, 'associ': 1830, 'lower': 1831, 'devic': 1832, 'contribut': 1833, 'balanc': 1834, 'frequent': 1835, 'degre': 1836, 'spell': 1837, 'wing': 1838, 'regret': 1839, 'frankli': 1840, 'forgiv': 1841, 'sake': 1842, 'print': 1843, 'lake': 1844, 'mass': 1845, 'thoma': 1846, 'betti': 1847, 'crack': 1848, 'unexpect': 1849, 'gordon': 1850, 'grown': 1851, 'categori': 1852, 'invit': 1853, 'construct': 1854, 'unfold': 1855, 'depend': 1856, 'amateur': 1857, 'walter': 1858, 'matur': 1859, 'condit': 1860, 'intellectu': 1861, 'anna': 1862, 'grew': 1863, 'honor': 1864, 'spectacular': 1865, 'sole': 1866, 'sudden': 1867, 'veteran': 1868, 'mirror': 1869, 'gift': 1870, 'grip': 1871, 'card': 1872, 'freedom': 1873, 'liner': 1874, 'overli': 1875, 'meanwhil': 1876, 'experienc': 1877, 'robin': 1878, 'demonstr': 1879, 'subtitl': 1880, 'circumst': 1881, 'section': 1882, 'drew': 1883, 'oliv': 1884, 'colour': 1885, 'crappi': 1886, 'sheriff': 1887, 'unabl': 1888, 'brilliantli': 1889, 'theori': 1890, 'pile': 1891, 'altern': 1892, 'sheer': 1893, 'matt': 1894, 'cook': 1895, 'path': 1896, 'laughter': 1897, 'parker': 1898, 'accident': 1899, 'treatment': 1900, 'relief': 1901, 'hall': 1902, 'lawyer': 1903, 'sinatra': 1904, 'defin': 1905, 'wander': 1906, 'hank': 1907, 'captiv': 1908, 'dragon': 1909, 'halloween': 1910, 'gratuit': 1911, 'moor': 1912, 'wound': 1913, 'k': 1914, 'unintent': 1915, 'kung': 1916, 'broadway': 1917, 'barbara': 1918, 'wayn': 1919, 'cowboy': 1920, 'jacki': 1921, 'canadian': 1922, 'surreal': 1923, 'spoof': 1924, 'winter': 1925, 'statement': 1926, 'gonna': 1927, 'treasur': 1928, 'cheer': 1929, 'fish': 1930, 'fare': 1931, 'compos': 1932, 'unrealist': 1933, 'emerg': 1934, 'victor': 1935, 'woodi': 1936, 'sensit': 1937, 'sympathet': 1938, 'neighbor': 1939, 'driven': 1940, 'ran': 1941, 'topic': 1942, 'overlook': 1943, 'authent': 1944, 'expos': 1945, 'menac': 1946, 'glass': 1947, 'chief': 1948, 'handsom': 1949, 'gross': 1950, 'ancient': 1951, 'michel': 1952, 'pleasant': 1953, 'feet': 1954, 'contemporari': 1955, 'network': 1956, 'russel': 1957, 'built': 1958, 'stranger': 1959, 'cinderella': 1960, 'nevertheless': 1961, 'comedian': 1962, 'gori': 1963, 'underr': 1964, 'blockbust': 1965, 'consider': 1966, 'endless': 1967, 'earn': 1968, 'letter': 1969, 'miser': 1970, 'switch': 1971, 'solv': 1972, 'brook': 1973, 'bullet': 1974, 'victoria': 1975, 'joseph': 1976, 'virgin': 1977, 'convict': 1978, 'edward': 1979, 'scale': 1980, 'alex': 1981, '0': 1982, 'scenario': 1983, 'cynic': 1984, 'chosen': 1985, 'sword': 1986, 'com': 1987, 'curs': 1988, 'gut': 1989, 'outrag': 1990, 'screenwrit': 1991, 'substanc': 1992, 'proper': 1993, 'wrap': 1994, 'uk': 1995, 'juli': 1996, 'driver': 1997, 'monkey': 1998, 'par': 1999, 'court': 2000, 'indic': 2001, 'remov': 2002, 'bird': 2003, 'grave': 2004, 'loser': 2005, 'advertis': 2006, 'inevit': 2007, 'consequ': 2008, 'naiv': 2009, 'roy': 2010, 'rental': 2011, 'nanci': 2012, 'bridg': 2013, 'brave': 2014, 'slap': 2015, 'le': 2016, 'invis': 2017, 'germani': 2018, 'fatal': 2019, 'anger': 2020, 'footbal': 2021, 'ador': 2022, 'provok': 2023, 'loui': 2024, 'alcohol': 2025, 'anderson': 2026, 'chan': 2027, 'stumbl': 2028, 'ryan': 2029, 'willi': 2030, 'professor': 2031, 'assassin': 2032, 'australian': 2033, 'sharp': 2034, 'patrick': 2035, 'bat': 2036, '1930': 2037, 'liber': 2038, 'strongli': 2039, 'saturday': 2040, 'lousi': 2041, 'ape': 2042, 'trilog': 2043, 'heck': 2044, 'amateurish': 2045, 'refresh': 2046, 'cell': 2047, 'eight': 2048, 'deni': 2049, 'sin': 2050, 'resid': 2051, 'san': 2052, 'justifi': 2053, 'vagu': 2054, 'defeat': 2055, 'terrifi': 2056, 'sympathi': 2057, 'creator': 2058, 'indi': 2059, 'reput': 2060, 'mini': 2061, 'task': 2062, 'endur': 2063, 'prevent': 2064, 'tediou': 2065, 'tabl': 2066, 'expert': 2067, 'offend': 2068, 'che': 2069, 'trial': 2070, 'imit': 2071, 'employ': 2072, 'basebal': 2073, 'rival': 2074, 'weekend': 2075, 'dig': 2076, 'pitch': 2077, 'beach': 2078, 'europ': 2079, 'max': 2080, 'complaint': 2081, 'fairi': 2082, 'murphi': 2083, 'format': 2084, 'purchas': 2085, 'risk': 2086, 'glimps': 2087, 'nois': 2088, 'tini': 2089, 'harsh': 2090, 'powel': 2091, 'reminisc': 2092, 'hype': 2093, 'bite': 2094, 'titan': 2095, 'asleep': 2096, 'prime': 2097, 'north': 2098, 'till': 2099, '14': 2100, 'strip': 2101, 'fals': 2102, 'descript': 2103, 'texa': 2104, 'destruct': 2105, 'revel': 2106, 'africa': 2107, 'surfac': 2108, 'uninterest': 2109, 'sitcom': 2110, 'semi': 2111, 'excess': 2112, 'spin': 2113, 'arrest': 2114, 'inner': 2115, 'dinosaur': 2116, 'argu': 2117, 'controversi': 2118, 'massiv': 2119, 'maintain': 2120, 'makeup': 2121, 'twin': 2122, 'hitchcock': 2123, 'ludicr': 2124, 'ideal': 2125, 'melodrama': 2126, 'expens': 2127, 'stare': 2128, 'reject': 2129, 'insist': 2130, 'kim': 2131, 'press': 2132, 'ala': 2133, 'host': 2134, 'atroci': 2135, 'subplot': 2136, 'nail': 2137, 'supernatur': 2138, 'erot': 2139, 'forest': 2140, 'ga': 2141, 'columbo': 2142, 'presum': 2143, 'dude': 2144, 'cant': 2145, 'notch': 2146, 'identifi': 2147, 'plagu': 2148, 'character': 2149, 'forgett': 2150, 'crude': 2151, 'method': 2152, 'guest': 2153, 'closer': 2154, 'lion': 2155, 'border': 2156, 'beast': 2157, 'princess': 2158, 'foster': 2159, 'landscap': 2160, 'ear': 2161, 'accus': 2162, 'bound': 2163, 'storytel': 2164, 'urban': 2165, 'pacino': 2166, 'aunt': 2167, 'previous': 2168, 'jungl': 2169, 'damag': 2170, 'birth': 2171, 'emma': 2172, 'chose': 2173, 'jess': 2174, 'propaganda': 2175, 'thirti': 2176, 'nude': 2177, 'doll': 2178, 'guid': 2179, 'mate': 2180, 'mainstream': 2181, 'warrior': 2182, 'whoever': 2183, '25': 2184, 'pet': 2185, 'poster': 2186, 'size': 2187, 'latest': 2188, 'gritti': 2189, 'deadli': 2190, 'merit': 2191, 'friday': 2192, 'cooper': 2193, 'upset': 2194, 'exact': 2195, 'sun': 2196, 'corps': 2197, 'ton': 2198, 'contest': 2199, 'warner': 2200, 'citizen': 2201, 'rough': 2202, 'settl': 2203, 'contact': 2204, '1990': 2205, 'blend': 2206, 'wilson': 2207, 'popul': 2208, 'buff': 2209, 'select': 2210, 'environ': 2211, 'rat': 2212, 'mgm': 2213, 'metal': 2214, 'pitt': 2215, 'widow': 2216, 'alic': 2217, 'overcom': 2218, 'bu': 2219, 'revolut': 2220, 'particip': 2221, 'ted': 2222, 'guilti': 2223, 'link': 2224, 'lift': 2225, 'moron': 2226, 'corpor': 2227, 'corner': 2228, 'accompani': 2229, 'johnson': 2230, 'exagger': 2231, 'afternoon': 2232, 'matrix': 2233, '1960': 2234, 'prostitut': 2235, 'hood': 2236, 'sincer': 2237, 'doom': 2238, 'multipl': 2239, 'friendli': 2240, 'leagu': 2241, 'clair': 2242, 'instal': 2243, 'holm': 2244, 'junk': 2245, 'grim': 2246, 'examin': 2247, 'aka': 2248, 'irish': 2249, 'string': 2250, 'advis': 2251, 'campi': 2252, 'lugosi': 2253, 'defend': 2254, 'hip': 2255, 'sunday': 2256, 'blah': 2257, 'shut': 2258, 'varieti': 2259, 'rachel': 2260, 'pro': 2261, 'confid': 2262, 'tight': 2263, 'icon': 2264, 'shake': 2265, 'mexican': 2266, 'medic': 2267, 'directli': 2268, 'jaw': 2269, 'sullivan': 2270, 'attach': 2271, 'goal': 2272, 'denni': 2273, 'breast': 2274, 'legendari': 2275, 'sarah': 2276, 'vietnam': 2277, 'courag': 2278, 'terrorist': 2279, 'bourn': 2280, 'truck': 2281, 'sentenc': 2282, 'dean': 2283, 'prior': 2284, 'duke': 2285, 'split': 2286, 'donald': 2287, 'un': 2288, 'yell': 2289, 'hong': 2290, 'proceed': 2291, 'behav': 2292, 'entri': 2293, 'nose': 2294, 'concentr': 2295, 'confess': 2296, 'jerk': 2297, 'crush': 2298, 'buri': 2299, 'everywher': 2300, 'gather': 2301, 'forth': 2302, 'stolen': 2303, 'borrow': 2304, 'lifetim': 2305, 'unconvinc': 2306, 'swim': 2307, 'turkey': 2308, 'lip': 2309, 'pan': 2310, 'spite': 2311, 'california': 2312, 'julia': 2313, 'deliveri': 2314, 'offici': 2315, 'hoffman': 2316, 'proud': 2317, 'flight': 2318, 'quest': 2319, 'reward': 2320, 'china': 2321, 'freeman': 2322, 'downright': 2323, 'fade': 2324, 'worthwhil': 2325, 'lazi': 2326, 'encourag': 2327, 'fabul': 2328, 'sink': 2329, 'inept': 2330, 'jon': 2331, 'sir': 2332, 'betray': 2333, 'jail': 2334, 'notori': 2335, 'bag': 2336, 'lisa': 2337, 'relev': 2338, 'bell': 2339, 'teeth': 2340, 'shower': 2341, 'cousin': 2342, 'survivor': 2343, 'retard': 2344, 'imageri': 2345, 'susan': 2346, 'storm': 2347, 'branagh': 2348, 'bride': 2349, 'quirki': 2350, 'alright': 2351, 'stab': 2352, 'hugh': 2353, 'tremend': 2354, 'toler': 2355, 'facial': 2356, 'mexico': 2357, 'shark': 2358, 'trade': 2359, 'finger': 2360, 'summari': 2361, 'ha': 2362, 'blown': 2363, 'bitter': 2364, 'von': 2365, 'pose': 2366, 'hyster': 2367, 'address': 2368, 'cruel': 2369, 'scheme': 2370, 'bone': 2371, 'christ': 2372, 'larri': 2373, 'afterward': 2374, 'ned': 2375, 'ron': 2376, 'screw': 2377, 'traci': 2378, 'feed': 2379, 'pursu': 2380, 'distinct': 2381, 'swear': 2382, 'snake': 2383, 'beg': 2384, 'tour': 2385, 'thumb': 2386, 'mechan': 2387, 'stomach': 2388, 'raw': 2389, 'chair': 2390, 'occas': 2391, 'obscur': 2392, 'photo': 2393, 'southern': 2394, 'argument': 2395, 'necessarili': 2396, 'resist': 2397, 'sidney': 2398, 'heavili': 2399, 'gruesom': 2400, 'render': 2401, 'cabin': 2402, 'hardi': 2403, 'holiday': 2404, 'chain': 2405, 'racist': 2406, 'philip': 2407, 'understood': 2408, 'india': 2409, 'indulg': 2410, 'satan': 2411, 'pregnant': 2412, 'lay': 2413, 'stalk': 2414, 'outfit': 2415, 'midnight': 2416, 'belov': 2417, 'obnoxi': 2418, 'integr': 2419, 'tongu': 2420, 'fourth': 2421, 'forgot': 2422, 'ticket': 2423, 'deeper': 2424, 'inhabit': 2425, 'slapstick': 2426, 'garden': 2427, '17': 2428, 'restor': 2429, 'magazin': 2430, 'carol': 2431, 'devot': 2432, 'brad': 2433, 'shoe': 2434, 'lincoln': 2435, 'incid': 2436, 'disbelief': 2437, 'elizabeth': 2438, 'guarante': 2439, 'divorc': 2440, 'anticip': 2441, 'maria': 2442, 'underground': 2443, 'lili': 2444, 'sandler': 2445, 'benefit': 2446, 'cring': 2447, 'princip': 2448, 'capit': 2449, 'amazingli': 2450, 'creation': 2451, 'explod': 2452, 'mildli': 2453, 'bbc': 2454, 'greater': 2455, 'slave': 2456, 'funnier': 2457, 'halfway': 2458, 'extraordinari': 2459, 'lesli': 2460, 'introduct': 2461, 'transfer': 2462, 'enhanc': 2463, 'wreck': 2464, 'text': 2465, 'extent': 2466, 'advantag': 2467, 'punish': 2468, 'overwhelm': 2469, 'tap': 2470, 'preview': 2471, 'deliber': 2472, 'error': 2473, 'dynam': 2474, 'lane': 2475, 'lo': 2476, 'east': 2477, 'horrif': 2478, 'plant': 2479, 'jessica': 2480, 'appli': 2481, 'homosexu': 2482, 'sophist': 2483, 'ensu': 2484, 'miscast': 2485, 'vincent': 2486, '2000': 2487, 'basi': 2488, 'miller': 2489, 'vacat': 2490, 'steel': 2491, 'spoken': 2492, 'mansion': 2493, 'bollywood': 2494, 'measur': 2495, 'sleazi': 2496, 'via': 2497, 'extend': 2498, 'reed': 2499, 'elev': 2500, 'uncomfort': 2501, 'mous': 2502, 'assign': 2503, 'alter': 2504, 'beer': 2505, 'breathtak': 2506, 'cathol': 2507, 'daili': 2508, 'conceiv': 2509, 'overact': 2510, 'savag': 2511, 'hippi': 2512, 'stanley': 2513, 'goofi': 2514, 'fix': 2515, 'dentist': 2516, 'melt': 2517, 'blair': 2518, 'sacrific': 2519, 'subsequ': 2520, 'inspector': 2521, 'oppos': 2522, 'nowaday': 2523, 'properli': 2524, 'succe': 2525, 'everyday': 2526, 'carpent': 2527, 'burt': 2528, 'neck': 2529, 'laura': 2530, 'massacr': 2531, 'circl': 2532, 'block': 2533, 'concert': 2534, 'mob': 2535, 'portrait': 2536, 'fay': 2537, 'pool': 2538, 'fallen': 2539, 'access': 2540, 'lesser': 2541, 'grey': 2542, 'christi': 2543, 'seagal': 2544, 'sinist': 2545, 'isol': 2546, 'chees': 2547, 'react': 2548, 'relax': 2549, 'competit': 2550, 'usa': 2551, 'jewish': 2552, 'jake': 2553, 'chop': 2554, 'spiritu': 2555, 'nonetheless': 2556, 'immens': 2557, 'appal': 2558, 'ironi': 2559, 'suitabl': 2560, 'lyric': 2561, '2006': 2562, 'nine': 2563, 'stink': 2564, 'creep': 2565, 'franchis': 2566, 'navi': 2567, 'needless': 2568, 'spring': 2569, 'adopt': 2570, 'user': 2571, 'shirt': 2572, 'sold': 2573, 'luci': 2574, 'retir': 2575, 'nut': 2576, 'showcas': 2577, 'rage': 2578, 'reduc': 2579, 'digit': 2580, 'zone': 2581, 'bath': 2582, 'uninspir': 2583, 'asham': 2584, 'jay': 2585, 'per': 2586, 'nurs': 2587, 'bulli': 2588, 'stanwyck': 2589, 'sutherland': 2590, 'amongst': 2591, '2001': 2592, 'illustr': 2593, '1940': 2594, 'oddli': 2595, 'broadcast': 2596, 'upper': 2597, 'laid': 2598, 'baker': 2599, 'disguis': 2600, 'fulfil': 2601, 'stylish': 2602, 'throat': 2603, 'aspir': 2604, 'brando': 2605, 'impli': 2606, 'pride': 2607, '18': 2608, 'neighborhood': 2609, 'thief': 2610, 'nobl': 2611, 'endear': 2612, 'em': 2613, 'wanna': 2614, 'pound': 2615, 'wwii': 2616, 'prop': 2617, '16': 2618, 'diseas': 2619, 'albeit': 2620, 'shoulder': 2621, 'coher': 2622, 'dawn': 2623, 'shift': 2624, 'tens': 2625, 'bett': 2626, 'distribut': 2627, 'cinematograph': 2628, 'bo': 2629, 'dinner': 2630, 'rochest': 2631, 'function': 2632, 'shout': 2633, 'poignant': 2634, 'wash': 2635, 'silenc': 2636, 'matthau': 2637, 'contract': 2638, 'snow': 2639, 'rebel': 2640, 'forti': 2641, 'knife': 2642, 'surf': 2643, 'instinct': 2644, 'mindless': 2645, 'reunion': 2646, 'horrend': 2647, 'eeri': 2648, 'widmark': 2649, 'cancel': 2650, 'heat': 2651, 'height': 2652, 'proof': 2653, 'elvira': 2654, 'silver': 2655, 'derek': 2656, 'chuck': 2657, 'internet': 2658, 'cannib': 2659, 'duti': 2660, 'henc': 2661, 'pie': 2662, 'etern': 2663, 'neat': 2664, 'spielberg': 2665, 'incoher': 2666, 'torn': 2667, 'repetit': 2668, 'premier': 2669, 'absorb': 2670, 'innov': 2671, 'greatli': 2672, 'musician': 2673, 'mill': 2674, 'alik': 2675, 'glori': 2676, 'elvi': 2677, 'nelson': 2678, 'crisi': 2679, 'homag': 2680, 'lovabl': 2681, 'infam': 2682, 'precis': 2683, 'bang': 2684, 'blank': 2685, 'burton': 2686, 'diamond': 2687, 'britain': 2688, 'fbi': 2689, 'racism': 2690, 'wealthi': 2691, 'horrifi': 2692, 'announc': 2693, 'redempt': 2694, 'trite': 2695, 'itali': 2696, 'parallel': 2697, 'flop': 2698, 'hammer': 2699, 'dedic': 2700, 'resolut': 2701, 'happili': 2702, 'ensembl': 2703, 'wilder': 2704, 'pat': 2705, 'helen': 2706, 'chaplin': 2707, 'streisand': 2708, 'plastic': 2709, 'oil': 2710, 'factori': 2711, 'disagre': 2712, 'carter': 2713, 'broke': 2714, 'mar': 2715, 'cube': 2716, 'conclud': 2717, 'triumph': 2718, 'st': 2719, 'row': 2720, 'chuckl': 2721, 'own': 2722, 'bush': 2723, 'rocket': 2724, 'march': 2725, 'weight': 2726, 'fighter': 2727, 'climb': 2728, 'vega': 2729, 'wherea': 2730, 'boot': 2731, 'enorm': 2732, 'thug': 2733, 'luca': 2734, 'lust': 2735, 'spare': 2736, 'sensibl': 2737, 'meaning': 2738, 'dump': 2739, 'dane': 2740, 'mst3k': 2741, 'unforgett': 2742, 'kurt': 2743, 'engin': 2744, 'butt': 2745, 'fifti': 2746, 'threat': 2747, 'dear': 2748, 'stress': 2749, 'caricatur': 2750, 'rap': 2751, 'arnold': 2752, 'difficulti': 2753, 'bobbi': 2754, 'adequ': 2755, 'brand': 2756, 'karloff': 2757, 'homeless': 2758, 'ralph': 2759, 'barri': 2760, 'polish': 2761, 'secretari': 2762, 'fest': 2763, 'arrog': 2764, 'ego': 2765, 'flynn': 2766, 'swing': 2767, 'elabor': 2768, 'hamlet': 2769, 'journalist': 2770, 'float': 2771, 'unbear': 2772, 'spike': 2773, 'resort': 2774, 'puppet': 2775, 'tool': 2776, 'induc': 2777, 'fanci': 2778, 'grate': 2779, 'conspiraci': 2780, 'arrang': 2781, 'simpson': 2782, 'exercis': 2783, 'cruis': 2784, 'muppet': 2785, 'phillip': 2786, 'choreograph': 2787, 'basement': 2788, 'guilt': 2789, 'pig': 2790, 'tribut': 2791, 'boll': 2792, 'scarecrow': 2793, 'document': 2794, 'puzzl': 2795, 'editor': 2796, 'fianc': 2797, 'slip': 2798, 'ham': 2799, 'layer': 2800, 'medium': 2801, 'korean': 2802, '24': 2803, 'stan': 2804, 'babe': 2805, 'ward': 2806, 'toilet': 2807, 'file': 2808, 'tower': 2809, 'item': 2810, 'larger': 2811, 'glover': 2812, 'superfici': 2813, 'catherin': 2814, 'minim': 2815, 'doc': 2816, 'philosoph': 2817, 'transit': 2818, 'denzel': 2819, 'portion': 2820, 'territori': 2821, 'orient': 2822, 'librari': 2823, 'inexplic': 2824, 'assur': 2825, 'slaughter': 2826, 'spark': 2827, 'persona': 2828, 'dorothi': 2829, 'jeremi': 2830, 'pg': 2831, 'sneak': 2832, 'financi': 2833, 'shi': 2834, 'jet': 2835, 'boredom': 2836, 'owe': 2837, 'wolf': 2838, 'ban': 2839, 'curti': 2840, 'walken': 2841, 'whale': 2842, 'metaphor': 2843, 'multi': 2844, 'hudson': 2845, 'backdrop': 2846, 'ambigu': 2847, 'profound': 2848, 'cusack': 2849, 'eleph': 2850, '2005': 2851, 'hack': 2852, 'ultra': 2853, 'union': 2854, 'elsewher': 2855, 'rave': 2856, 'implaus': 2857, 'stiff': 2858, 'notion': 2859, 'birthday': 2860, 'viru': 2861, 'gadget': 2862, 'squar': 2863, 'urg': 2864, 'hawk': 2865, 'reader': 2866, 'distanc': 2867, 'poison': 2868, 'deriv': 2869, 'canada': 2870, 'slight': 2871, 'bibl': 2872, 'afford': 2873, 'disc': 2874, '1st': 2875, 'eastwood': 2876, 'pad': 2877, 'eva': 2878, 'lloyd': 2879, 'superhero': 2880, 'newspap': 2881, 'montag': 2882, 'button': 2883, 'essenc': 2884, 'sadist': 2885, 'drown': 2886, 'heston': 2887, 'cure': 2888, 'skit': 2889, 'spread': 2890, 'huh': 2891, 'health': 2892, 'charisma': 2893, 'restaur': 2894, 'dealt': 2895, 'peak': 2896, 'maniac': 2897, 'companion': 2898, 'muslim': 2899, 'invest': 2900, 'fetch': 2901, 'lab': 2902, 'godfath': 2903, 'estat': 2904, 'scoobi': 2905, 'gradual': 2906, 'subtleti': 2907, 'ritter': 2908, 'gothic': 2909, 'servant': 2910, 'kane': 2911, 'cup': 2912, 'tea': 2913, 'countless': 2914, 'miik': 2915, 'alli': 2916, 'charismat': 2917, 'electr': 2918, 'elect': 2919, 'heroic': 2920, 'briefli': 2921, 'iii': 2922, 'salli': 2923, 'tender': 2924, 'cole': 2925, 'bud': 2926, 'neil': 2927, 'resourc': 2928, 'toss': 2929, 'admittedli': 2930, 'wannab': 2931, 'reel': 2932, 'nuanc': 2933, 'grandmoth': 2934, 'ingredi': 2935, 'punk': 2936, 'stronger': 2937, 'dawson': 2938, 'stood': 2939, 'shall': 2940, 'poverti': 2941, 'pit': 2942, 'reev': 2943, 'mild': 2944, 'label': 2945, 'pauli': 2946, 'mafia': 2947, 'gate': 2948, 'kubrick': 2949, 'carrey': 2950, 'assault': 2951, 'ian': 2952, 'astair': 2953, 'fond': 2954, 'outcom': 2955, 'smash': 2956, 'cardboard': 2957, 'tag': 2958, 'cox': 2959, 'useless': 2960, 'terri': 2961, 'updat': 2962, 'easier': 2963, 'burst': 2964, 'bakshi': 2965, 'smooth': 2966, 'vulner': 2967, 'rex': 2968, 'vari': 2969, 'resolv': 2970, 'fist': 2971, 'exchang': 2972, 'qualifi': 2973, '2002': 2974, 'sketch': 2975, 'divers': 2976, 'increasingli': 2977, 'melodramat': 2978, 'samurai': 2979, 'coincid': 2980, 'conveni': 2981, 'suspend': 2982, 'insert': 2983, 'brillianc': 2984, 'blast': 2985, 'be': 2986, 'scratch': 2987, 'templ': 2988, 'reynold': 2989, 'tame': 2990, 'luckili': 2991, 'farm': 2992, 'gotta': 2993, 'meat': 2994, 'seventi': 2995, 'coach': 2996, 'matthew': 2997, 'pin': 2998, 'strictli': 2999, 'jami': 3000, 'ambiti': 3001, 'nuclear': 3002, 'walker': 3003, 'fisher': 3004, 'hamilton': 3005, 'soprano': 3006, 'spooki': 3007, 'ninja': 3008, 'joey': 3009, 'eccentr': 3010, 'brosnan': 3011, 'convolut': 3012, 'closet': 3013, 'recreat': 3014, 'cave': 3015, 'revers': 3016, 'monk': 3017, 'empir': 3018, 'timeless': 3019, 'kudo': 3020, 'grasp': 3021, 'discoveri': 3022, 'instantli': 3023, 'clock': 3024, 'struck': 3025, 'butcher': 3026, 'worthless': 3027, 'importantli': 3028, 'gray': 3029, 'selfish': 3030, 'miracl': 3031, 'partli': 3032, 'norman': 3033, 'inconsist': 3034, 'declar': 3035, 'cliff': 3036, 'evok': 3037, 'mitchel': 3038, 'fifteen': 3039, 'communist': 3040, 'clown': 3041, 'sidekick': 3042, 'sloppi': 3043, 'wipe': 3044, 'eighti': 3045, 'pal': 3046, 'seller': 3047, 'bleak': 3048, 'chew': 3049, 'aforement': 3050, 'destin': 3051, 'debat': 3052, 'superbl': 3053, 'australia': 3054, 'ho': 3055, 'cheek': 3056, 'enthusiast': 3057, 'flawless': 3058, 'stoog': 3059, 'websit': 3060, 'seed': 3061, 'piano': 3062, '45': 3063, 'psychiatrist': 3064, 'lifestyl': 3065, 'farc': 3066, 'pressur': 3067, 'splatter': 3068, 'bash': 3069, 'regardless': 3070, 'drivel': 3071, 'kitchen': 3072, 'dash': 3073, 'wick': 3074, 'directori': 3075, 'slice': 3076, 'anni': 3077, 'incompet': 3078, 'akshay': 3079, 'soviet': 3080, 'emili': 3081, 'dire': 3082, 'abc': 3083, 'wrestl': 3084, 'judi': 3085, 'jar': 3086, 'flower': 3087, 'seduc': 3088, 'cameron': 3089, 'glow': 3090, 'recov': 3091, 'duo': 3092, 'cia': 3093, 'artifici': 3094, 'lou': 3095, 'ken': 3096, 'chapter': 3097, 'increas': 3098, 'distant': 3099, 'dave': 3100, 'suppli': 3101, 'beaten': 3102, 'prize': 3103, 'curios': 3104, 'helicopt': 3105, 'pleasantli': 3106, 'boil': 3107, 'doo': 3108, 'blob': 3109, 'mann': 3110, 'cagney': 3111, 'favour': 3112, 'craven': 3113, 'goldberg': 3114, 'ranger': 3115, 'psychot': 3116, 'combat': 3117, 'craig': 3118, 'ellen': 3119, 'panic': 3120, 'drunken': 3121, 'glenn': 3122, 'turner': 3123, 'web': 3124, 'laurel': 3125, 'hop': 3126, 'eleg': 3127, 'francisco': 3128, 'splendid': 3129, 'perri': 3130, 'wizard': 3131, 'graduat': 3132, 'alexand': 3133, 'min': 3134, '20th': 3135, 'flip': 3136, 'plausibl': 3137, 'rid': 3138, 'gentl': 3139, 'fx': 3140, 'hatr': 3141, 'ruth': 3142, 'falk': 3143, 'slightest': 3144, 'philosophi': 3145, 'greek': 3146, 'modesti': 3147, 'gandhi': 3148, 'shortli': 3149, 'jealou': 3150, 'we': 3151, 'legal': 3152, 'futurist': 3153, 'harm': 3154, 'dracula': 3155, 'holi': 3156, 'unpleas': 3157, 'ocean': 3158, 'knight': 3159, 'tall': 3160, 'manhattan': 3161, 'preciou': 3162, 'lend': 3163, 'fund': 3164, 'felix': 3165, 'reviv': 3166, 'tank': 3167, 'scientif': 3168, 'overdon': 3169, 'thread': 3170, 'digniti': 3171, 'nod': 3172, 'childish': 3173, 'forbidden': 3174, 'ami': 3175, 'explicit': 3176, 'giallo': 3177, 'mock': 3178, 'bless': 3179, 'margaret': 3180, 'broad': 3181, 'unwatch': 3182, 'mel': 3183, 'thick': 3184, 'torment': 3185, 'yesterday': 3186, 'eve': 3187, 'nerv': 3188, 'elderli': 3189, '99': 3190, 'awe': 3191, 'repeatedli': 3192, 'pirat': 3193, 'awaken': 3194, '2004': 3195, 'fever': 3196, 'verhoeven': 3197, 'romero': 3198, 'griffith': 3199, 'ah': 3200, 'royal': 3201, 'absenc': 3202, 'acclaim': 3203, 'publish': 3204, 'bin': 3205, 'roman': 3206, 'automat': 3207, 'uniform': 3208, 'timothi': 3209, 'eas': 3210, 'rivet': 3211, 'launch': 3212, 'ambit': 3213, 'stiller': 3214, 'politician': 3215, 'custom': 3216, 'kay': 3217, 'lean': 3218, 'pierc': 3219, 'warren': 3220, 'darker': 3221, 'homicid': 3222, 'purpl': 3223, 'phrase': 3224, 'bathroom': 3225, 'stinker': 3226, 'transport': 3227, 'pulp': 3228, 'sunshin': 3229, 'antic': 3230, 'wallac': 3231, 'foul': 3232, 'termin': 3233, 'gabriel': 3234, 'tomato': 3235, 'crook': 3236, 'juvenil': 3237, 'donna': 3238, 'coloni': 3239, 'q': 3240, 'ought': 3241, 'choreographi': 3242, 'pray': 3243, 'li': 3244, 'revolutionari': 3245, 'sixti': 3246, 'rambo': 3247, 'contrari': 3248, 'eyr': 3249, 'viciou': 3250, 'horrid': 3251, 'marin': 3252, 'prom': 3253, 'kenneth': 3254, 'karen': 3255, 'album': 3256, 'hollow': 3257, 'awak': 3258, 'saint': 3259, 'packag': 3260, 'brazil': 3261, '2003': 3262, 'evolv': 3263, 'conserv': 3264, 'dose': 3265, 'stole': 3266, 'blade': 3267, 'beatti': 3268, 'overr': 3269, 'boast': 3270, 'candid': 3271, 'defi': 3272, 'ramon': 3273, 'mummi': 3274, 'kapoor': 3275, 'option': 3276, 'twelv': 3277, 'ireland': 3278, 'nerd': 3279, 'mildr': 3280, 'natali': 3281, 'detract': 3282, 'kirk': 3283, 'flame': 3284, 'astonish': 3285, 'trio': 3286, 'altman': 3287, 'fulci': 3288, 'protest': 3289, 'collabor': 3290, 'funer': 3291, 'jazz': 3292, 'global': 3293, 'confirm': 3294, 'bull': 3295, 'delici': 3296, 'blake': 3297, 'shade': 3298, 'whip': 3299, 'spit': 3300, 'nicholson': 3301, 'yellow': 3302, 'bottl': 3303, 'tommi': 3304, 'audio': 3305, 'leap': 3306, 'mystic': 3307, 'racial': 3308, 'destini': 3309, 'enterpris': 3310, 'reunit': 3311, 'vivid': 3312, 'popcorn': 3313, 'todd': 3314, 'harder': 3315, 'visibl': 3316, 'neo': 3317, 'staff': 3318, 'meaningless': 3319, 'altogeth': 3320, 'merci': 3321, 'adolesc': 3322, 'enchant': 3323, 'inherit': 3324, 'threw': 3325, 'fonda': 3326, 'pseudo': 3327, 'bedroom': 3328, 'swedish': 3329, 'crocodil': 3330, 'bust': 3331, 'uneven': 3332, 'jew': 3333, 'befriend': 3334, 'lawrenc': 3335, 'moodi': 3336, 'madonna': 3337, 'ruthless': 3338, 'await': 3339, 'suspici': 3340, 'fanat': 3341, 'decor': 3342, 'tip': 3343, 'voight': 3344, 'atlanti': 3345, 'leonard': 3346, 'exhibit': 3347, 'roommat': 3348, 'wire': 3349, 'respond': 3350, 'reserv': 3351, 'kennedi': 3352, 'lemmon': 3353, 'synopsi': 3354, 'edi': 3355, 'dimens': 3356, 'voyag': 3357, 'rural': 3358, 'audit': 3359, 'unsettl': 3360, 'ventur': 3361, 'bargain': 3362, 'garner': 3363, 'centr': 3364, 'abysm': 3365, 'bradi': 3366, 'holli': 3367, '2007': 3368, 'incident': 3369, 'palma': 3370, 'chao': 3371, 'clint': 3372, 'clumsi': 3373, 'carl': 3374, 'bold': 3375, 'cuba': 3376, 'acknowledg': 3377, '2nd': 3378, 'neglect': 3379, 'troop': 3380, 'trail': 3381, 'hart': 3382, 'nearbi': 3383, 'lit': 3384, 'daddi': 3385, 'characterist': 3386, 'ant': 3387, 'tiger': 3388, 'imperson': 3389, 'echo': 3390, 'poetic': 3391, 'mall': 3392, 'cd': 3393, 'immigr': 3394, 'timon': 3395, 'versu': 3396, 'wealth': 3397, 'cari': 3398, 'elimin': 3399, 'humili': 3400, 'infect': 3401, 'mickey': 3402, 'saga': 3403, 'marshal': 3404, 'pun': 3405, 'domest': 3406, 'prejudic': 3407, 'collaps': 3408, 'repuls': 3409, 'jeffrey': 3410, 'homer': 3411, 'celluloid': 3412, 'solo': 3413, 'mistaken': 3414, 'paus': 3415, 'assembl': 3416, 'gear': 3417, 'sore': 3418, 'coffe': 3419, 'ginger': 3420, 'inan': 3421, 'inappropri': 3422, 'cake': 3423, '1996': 3424, 'equip': 3425, 'leon': 3426, 'coat': 3427, 'harvey': 3428, 'promin': 3429, 'undoubtedli': 3430, 'apolog': 3431, 'pant': 3432, 'milk': 3433, 'chest': 3434, 'hbo': 3435, 'olivi': 3436, 'interrupt': 3437, 'tribe': 3438, 'maggi': 3439, 'airplan': 3440, 'pen': 3441, 'trace': 3442, 'primari': 3443, 'devast': 3444, 'retain': 3445, 'colleagu': 3446, 'solut': 3447, 'brooklyn': 3448, 'jenni': 3449, 'highest': 3450, 'humbl': 3451, 'florida': 3452, 'aveng': 3453, 'colonel': 3454, 'institut': 3455, 'pot': 3456, 'polanski': 3457, 'embrac': 3458, 'vulgar': 3459, 'exot': 3460, 'instant': 3461, 'furthermor': 3462, 'consum': 3463, 'smaller': 3464, 'dian': 3465, 'principl': 3466, 'outer': 3467, 'linda': 3468, 'rick': 3469, 'godzilla': 3470, 'cope': 3471, '3rd': 3472, 'descend': 3473, 'bowl': 3474, 'dutch': 3475, 'illog': 3476, 'poke': 3477, 'ya': 3478, 'disabl': 3479, 'strain': 3480, '1999': 3481, 'gender': 3482, 'wive': 3483, 'seduct': 3484, 'sale': 3485, 'hal': 3486, 'mixtur': 3487, 'secondli': 3488, 'primarili': 3489, 'scope': 3490, 'devoid': 3491, 'gundam': 3492, 'lol': 3493, 'yard': 3494, 'glamor': 3495, 'rabbit': 3496, 'predecessor': 3497, 'blatant': 3498, 'inferior': 3499, 'cue': 3500, 'dud': 3501, 'gloriou': 3502, 'bubbl': 3503, 'vast': 3504, 'dive': 3505, 'beneath': 3506, 'museum': 3507, 'et': 3508, 'garbo': 3509, 'shelf': 3510, 'talki': 3511, 'invas': 3512, 'senseless': 3513, 'countrysid': 3514, 'simplist': 3515, 'hideou': 3516, 'disjoint': 3517, 'myer': 3518, 'pearl': 3519, 'alert': 3520, 'casual': 3521, 'arab': 3522, 'streep': 3523, 'breed': 3524, 'alfr': 3525, 'aggress': 3526, 'shirley': 3527, 'domino': 3528, 'z': 3529, 'trademark': 3530, 'april': 3531, 'grinch': 3532, 'obtain': 3533, 'oz': 3534, 'rendit': 3535, 'experiment': 3536, 'disgrac': 3537, 'robberi': 3538, 'unhappi': 3539, 'stellar': 3540, 'stack': 3541, 'applaud': 3542, 'maci': 3543, 'hopeless': 3544, 'khan': 3545, 'acid': 3546, 'robinson': 3547, 'defens': 3548, 'hardcor': 3549, 'stir': 3550, 'slide': 3551, 'sh': 3552, 'mail': 3553, 'vanish': 3554, 'loyal': 3555, 'boom': 3556, 'illeg': 3557, 'uwe': 3558, 'mayor': 3559, 'declin': 3560, 'rifl': 3561, 'counter': 3562, 'recruit': 3563, 'emphasi': 3564, 'berlin': 3565, 'tempt': 3566, 'craze': 3567, 'tenant': 3568, 'grandfath': 3569, 'psychic': 3570, 'span': 3571, 'amanda': 3572, 'incomprehens': 3573, 'soccer': 3574, 'wont': 3575, 'hartley': 3576, 'dicken': 3577, 'spider': 3578, 'blew': 3579, 'fri': 3580, 'topless': 3581, 'diana': 3582, 'scroog': 3583, 'dismiss': 3584, 'niro': 3585, 'ration': 3586, 'goer': 3587, 'lumet': 3588, 'resurrect': 3589, 'shaw': 3590, 'revolt': 3591, 'faster': 3592, 'sibl': 3593, 'trashi': 3594, 'intim': 3595, 'ethnic': 3596, 'sympath': 3597, 'riot': 3598, 'bitch': 3599, 'justin': 3600, 'shed': 3601, 'woo': 3602, 'porno': 3603, 'wet': 3604, 'parad': 3605, 'region': 3606, 'commend': 3607, 'steam': 3608, 'immort': 3609, 'patriot': 3610, 'honesti': 3611, 'wheel': 3612, 'farmer': 3613, 'hesit': 3614, 'gap': 3615, 'dealer': 3616, 'nephew': 3617, 'eager': 3618, 'enlighten': 3619, 'slick': 3620, 'partial': 3621, 'ensur': 3622, 'feminist': 3623, 'worm': 3624, '00': 3625, 'ballet': 3626, 'unreal': 3627, 'weakest': 3628, 'biographi': 3629, 'jonathan': 3630, 'mario': 3631, 'choru': 3632, 'andr': 3633, 'hopper': 3634, 'lena': 3635, 'wendi': 3636, 'rider': 3637, 'safeti': 3638, 'skull': 3639, 'repress': 3640, 'prequel': 3641, 'hung': 3642, 'franco': 3643, 'mutant': 3644, 'wore': 3645, 'composit': 3646, 'charlott': 3647, 'kingdom': 3648, 'sandra': 3649, 'nostalg': 3650, 'sappi': 3651, 'morri': 3652, 'confin': 3653, 'psychopath': 3654, 'similarli': 3655, 'properti': 3656, 'leo': 3657, 'blunt': 3658, 'snap': 3659, 'vice': 3660, 'util': 3661, 'owen': 3662, 'victori': 3663, 'macarthur': 3664, 'miseri': 3665, 'cg': 3666, 'montana': 3667, 'whoopi': 3668, 'farrel': 3669, 'dust': 3670, 'exit': 3671, 'speci': 3672, 'bumbl': 3673, 'compens': 3674, 'bonu': 3675, 'campbel': 3676, 'kyle': 3677, '1972': 3678, 'latin': 3679, 'tad': 3680, 'strand': 3681, 'rope': 3682, 'thru': 3683, 'drum': 3684, 'deed': 3685, 'hyde': 3686, 'bergman': 3687, 'snl': 3688, 'valuabl': 3689, 'del': 3690, 'drain': 3691, 'nervou': 3692, 'recycl': 3693, 'repli': 3694, 'heartbreak': 3695, 'tail': 3696, 'rambl': 3697, 'rocki': 3698, 'dalton': 3699, 'pattern': 3700, 'compass': 3701, 'despair': 3702, 'acquir': 3703, 'bow': 3704, 'emperor': 3705, 'bleed': 3706, 'oppress': 3707, 'tonight': 3708, '35': 3709, 'mistress': 3710, 'rotten': 3711, 'contempl': 3712, 'airport': 3713, 'downhil': 3714, 'romp': 3715, 'roth': 3716, 'rapist': 3717, 'slug': 3718, 'olli': 3719, 'gimmick': 3720, 'orson': 3721, 'martian': 3722, 'radic': 3723, 'wacki': 3724, 'percept': 3725, 'carradin': 3726, 'pour': 3727, 'gal': 3728, 'da': 3729, 'chess': 3730, 'mislead': 3731, 'paltrow': 3732, 'banal': 3733, 'heal': 3734, 'dazzl': 3735, 'arc': 3736, '1983': 3737, 'arguabl': 3738, 'pervers': 3739, 'slash': 3740, 'preach': 3741, 'belt': 3742, 'edgar': 3743, 'melodi': 3744, 'stilt': 3745, 'tooth': 3746, 'attorney': 3747, 'tackl': 3748, 'unpredict': 3749, 'pursuit': 3750, 'shelley': 3751, 'programm': 3752, 'taught': 3753, 'pervert': 3754, 'champion': 3755, 'vocal': 3756, 'sensat': 3757, 'uplift': 3758, 'mesmer': 3759, 'gambl': 3760, 'chicken': 3761, 'rubi': 3762, 'employe': 3763, 'graham': 3764, 'passeng': 3765, 'bela': 3766, 'cleverli': 3767, 'tiresom': 3768, 'plight': 3769, 'franki': 3770, 'maid': 3771, 'raymond': 3772, 'vengeanc': 3773, 'virginia': 3774, 'conneri': 3775, 'dixon': 3776, 'orang': 3777, 'poem': 3778, 'duval': 3779, 'closest': 3780, 'marti': 3781, 'extens': 3782, 'suffic': 3783, 'gerard': 3784, 'inject': 3785, 'whine': 3786, 'calm': 3787, 'swallow': 3788, 'climact': 3789, 'quarter': 3790, 'numb': 3791, 'bay': 3792, 'paranoia': 3793, 'secretli': 3794, 'clone': 3795, 'outing': 3796, 'engross': 3797, 'mute': 3798, 'crystal': 3799, 'convincingli': 3800, 'abraham': 3801, 'tube': 3802, 'volum': 3803, '1968': 3804, 'profan': 3805, 'lundgren': 3806, 'monologu': 3807, 'giggl': 3808, 'amitabh': 3809, 'habit': 3810, 'iran': 3811, 'pokemon': 3812, 'yawn': 3813, 'sirk': 3814, 'scottish': 3815, 'lowest': 3816, 'fed': 3817, 'austen': 3818, 'bend': 3819, 'ethan': 3820, 'frankenstein': 3821, 'chicago': 3822, 'meander': 3823, 'backward': 3824, 'plod': 3825, 'spock': 3826, 'nichola': 3827, 'surpass': 3828, 'trend': 3829, 'underst': 3830, 'richardson': 3831, 'profess': 3832, 'grotesqu': 3833, 'expand': 3834, 'dispos': 3835, 'linger': 3836, 'earl': 3837, 'im': 3838, 'taxi': 3839, 'poetri': 3840, 'franci': 3841, 'septemb': 3842, 'abort': 3843, 'junior': 3844, 'cannon': 3845, 'lure': 3846, 'hum': 3847, 'household': 3848, 'descent': 3849, 'spoke': 3850, 'mundan': 3851, 'compliment': 3852, 'econom': 3853, 'waitress': 3854, 'rant': 3855, 'tourist': 3856, 'literatur': 3857, 'catchi': 3858, 'simplic': 3859, 'dysfunct': 3860, 'stallon': 3861, 'greedi': 3862, 'der': 3863, 'nostalgia': 3864, 'instrument': 3865, 'eugen': 3866, 'myth': 3867, 'sue': 3868, 'rubber': 3869, 'muddl': 3870, 'mortal': 3871, 'insur': 3872, 'equival': 3873, 'coast': 3874, 'duck': 3875, 'june': 3876, 'flee': 3877, 'dictat': 3878, 'cent': 3879, 'mankind': 3880, 'randi': 3881, 'recognit': 3882, 'damon': 3883, 'map': 3884, 'eaten': 3885, 'recognis': 3886, 'phoni': 3887, 'dement': 3888, 'occupi': 3889, 'hello': 3890, 'stale': 3891, 'crucial': 3892, 'firstli': 3893, 'sissi': 3894, 'alongsid': 3895, 'deaf': 3896, 'bacal': 3897, 'molli': 3898, 'lang': 3899, 'carel': 3900, 'omen': 3901, 'furi': 3902, 'louis': 3903, 'phantom': 3904, 'irrelev': 3905, 'buffalo': 3906, 'labor': 3907, 'freez': 3908, 'ashley': 3909, 'onlin': 3910, 'likewis': 3911, 'twilight': 3912, 'bump': 3913, 'lengthi': 3914, 'dreari': 3915, 'wisdom': 3916, 'drake': 3917, 'loyalti': 3918, 'blackmail': 3919, 'heel': 3920, 'cyborg': 3921, 'bike': 3922, 'grayson': 3923, 'distinguish': 3924, 'reign': 3925, 'daisi': 3926, 'rooney': 3927, 'antwon': 3928, 'newli': 3929, 'biko': 3930, 'rude': 3931, '1973': 3932, 'damm': 3933, 'prey': 3934, 'baddi': 3935, 'emphas': 3936, 'chronicl': 3937, 'proce': 3938, 'worn': 3939, 'ridden': 3940, 'startl': 3941, 'butler': 3942, 'unorigin': 3943, 'provoc': 3944, 'inher': 3945, 'analysi': 3946, 'sailor': 3947, 'boxer': 3948, 'nineti': 3949, 'keith': 3950, 'exposur': 3951, 'approv': 3952, 'barrymor': 3953, 'pink': 3954, 'tunnel': 3955, 'interior': 3956, 'basketbal': 3957, 'attribut': 3958, 'incorpor': 3959, 'vein': 3960, 'predat': 3961, 'millionair': 3962, 'undeni': 3963, 'drift': 3964, 'robbin': 3965, 'underli': 3966, 'indiffer': 3967, 'barrel': 3968, 'belushi': 3969, 'fleet': 3970, 'meyer': 3971, 'degrad': 3972, 'walsh': 3973, 'condemn': 3974, 'mighti': 3975, 'unrel': 3976, 'improvis': 3977, 'stalker': 3978, 'nicol': 3979, 'meg': 3980, 'julian': 3981, 'er': 3982, 'hypnot': 3983, 'elm': 3984, 'simmon': 3985, 'substitut': 3986, 'carla': 3987, 'othello': 3988, 'mormon': 3989, 'bunni': 3990, 'unawar': 3991, 'hay': 3992, 'mtv': 3993, 'disord': 3994, 'edgi': 3995, 'firm': 3996, 'roof': 3997, 'watson': 3998, 'nyc': 3999, 'priceless': 4000, 'warmth': 4001, 'lampoon': 4002, 'alison': 4003, 'reid': 4004, 'marion': 4005, 'errol': 4006, 'shove': 4007, 'vital': 4008, 'enthusiasm': 4009, 'exquisit': 4010, 'palac': 4011, 'agenda': 4012, 'greed': 4013, 'dolph': 4014, 'alarm': 4015, 'novak': 4016, '3d': 4017, 'rukh': 4018, 'drip': 4019, 'campaign': 4020, 'petti': 4021, 'coup': 4022, 'championship': 4023, 'eastern': 4024, 'thompson': 4025, 'simultan': 4026, 'session': 4027, 'nun': 4028, 'what': 4029, 'valentin': 4030, 'gestur': 4031, 'ponder': 4032, 'testament': 4033, '1933': 4034, 'randomli': 4035, 'showdown': 4036, 'minimum': 4037, 'pamela': 4038, 'spain': 4039, '13th': 4040, 'peril': 4041, 'beatl': 4042, 'glanc': 4043, 'angela': 4044, 'cassidi': 4045, 'peck': 4046, 'profit': 4047, 'orlean': 4048, 'iraq': 4049, 'preserv': 4050, 'sergeant': 4051, 'zizek': 4052, 'crown': 4053, 'unleash': 4054, 'distort': 4055, 'israel': 4056, 'fido': 4057, 'represent': 4058, 'perpetu': 4059, 'climat': 4060, 'exposit': 4061, 'bro': 4062, 'jan': 4063, 'realm': 4064, 'calib': 4065, 'stake': 4066, 'contradict': 4067, 'han': 4068, 'scotland': 4069, '1984': 4070, 'quinn': 4071, 'cream': 4072, 'rout': 4073, 'valley': 4074, 'travesti': 4075, 'buster': 4076, 'regist': 4077, 'gentleman': 4078, 'shootout': 4079, 'sabrina': 4080, 'empathi': 4081, 'reson': 4082, 'wig': 4083, 'mon': 4084, 'unimagin': 4085, 'restrain': 4086, 'miyazaki': 4087, 'kurosawa': 4088, 'crow': 4089, 'brenda': 4090, 'crawl': 4091, 'cooki': 4092, 'din': 4093, 'stroke': 4094, 'abomin': 4095, 'derang': 4096, 'sucker': 4097, 'businessman': 4098, 'geek': 4099, 'soderbergh': 4100, 'ross': 4101, 'demis': 4102, 'distress': 4103, 'dana': 4104, 'josh': 4105, 'sammi': 4106, 'wax': 4107, 'unsatisfi': 4108, 'censor': 4109, 'darren': 4110, 'unseen': 4111, 'warrant': 4112, 'traumat': 4113, 'pole': 4114, 'spacey': 4115, 'shoddi': 4116, 'monoton': 4117, 'absent': 4118, 'delic': 4119, 'passabl': 4120, 'fuller': 4121, 'pretens': 4122, 'ustinov': 4123, 'tacki': 4124, '1987': 4125, 'cloud': 4126, 'painter': 4127, 'crawford': 4128, 'mclaglen': 4129, 'compromis': 4130, 'baldwin': 4131, 'shaki': 4132, 'perceiv': 4133, 'meryl': 4134, 'greg': 4135, '1997': 4136, 'femm': 4137, 'stargat': 4138, 'deniro': 4139, 'uncov': 4140, 'correctli': 4141, 'norm': 4142, 'tech': 4143, 'tarantino': 4144, 'verbal': 4145, 'sid': 4146, 'anchor': 4147, 'accuraci': 4148, 'antonioni': 4149, '1993': 4150, 'jewel': 4151, 'primit': 4152, 'judgment': 4153, 'clash': 4154, 'furiou': 4155, 'austin': 4156, 'seal': 4157, 'unravel': 4158, 'deceas': 4159, 'wholli': 4160, 'exclus': 4161, 'kumar': 4162, 'reluct': 4163, 'dee': 4164, 'polici': 4165, 'fog': 4166, 'fenc': 4167, 'nathan': 4168, 'expedit': 4169, 'valid': 4170, 'click': 4171, 'dreck': 4172, 'conduct': 4173, 'joel': 4174, 'clerk': 4175, 'ritual': 4176, 'behold': 4177, '2008': 4178, 'temper': 4179, 'debt': 4180, 'seldom': 4181, 'trait': 4182, 'logan': 4183, 'malon': 4184, 'slam': 4185, '1995': 4186, 'unfair': 4187, '1971': 4188, 'sand': 4189, 'roller': 4190, 'darn': 4191, 'fart': 4192, 'crippl': 4193, 'sustain': 4194, 'sheet': 4195, 'hallucin': 4196, 'fabric': 4197, 'nicola': 4198, 'mode': 4199, 'pocket': 4200, 'tax': 4201, 'bake': 4202, '3000': 4203, 'wang': 4204, 'fought': 4205, 'patienc': 4206, 'wretch': 4207, 'enforc': 4208, 'sunni': 4209, 'alec': 4210, 'murray': 4211, 'shanghai': 4212, 'vanc': 4213, 'scriptwrit': 4214, 'fundament': 4215, 'schedul': 4216, 'squad': 4217, 'isabel': 4218, 'runner': 4219, 'grief': 4220, 'robber': 4221, 'critiqu': 4222, 'pete': 4223, 'despis': 4224, 'rita': 4225, 'sweep': 4226, 'preposter': 4227, 'exhaust': 4228, 'helpless': 4229, 'divid': 4230, 'bias': 4231, 'tactic': 4232, 'bridget': 4233, 'legaci': 4234, 'clau': 4235, 'outlin': 4236, 'guitar': 4237, 'stark': 4238, 'shell': 4239, 'canyon': 4240, 'conscious': 4241, 'preston': 4242, 'phil': 4243, 'penni': 4244, 'technicolor': 4245, 'soup': 4246, 'stuart': 4247, 'boyl': 4248, 'liberti': 4249, 'sentinel': 4250, 'passag': 4251, 'cigarett': 4252, 'vomit': 4253, 'lacklust': 4254, 'invad': 4255, 'restrict': 4256, 'rehash': 4257, 'connor': 4258, 'consciou': 4259, 'kansa': 4260, 'marc': 4261, 'implic': 4262, 'rear': 4263, 'culmin': 4264, 'sugar': 4265, 'propos': 4266, 'flair': 4267, 'alley': 4268, 'delv': 4269, 'inabl': 4270, 'palanc': 4271, 'newman': 4272, 'bloom': 4273, 'alicia': 4274, 'jodi': 4275, 'russia': 4276, 'drove': 4277, 'jacket': 4278, 'unexpectedli': 4279, 'gregori': 4280, 'sniper': 4281, 'downey': 4282, 'agenc': 4283, 'lush': 4284, 'feat': 4285, 'tripe': 4286, 'mccoy': 4287, 'vet': 4288, 'aesthet': 4289, 'pale': 4290, 'delet': 4291, 'ladder': 4292, 'asylum': 4293, 'arrow': 4294, 'awhil': 4295, '22': 4296, 'cap': 4297, 'wrench': 4298, 'tendenc': 4299, 'behaviour': 4300, 'rod': 4301, 'improb': 4302, 'kolchak': 4303, '1936': 4304, 'rampag': 4305, 'horn': 4306, 'bacon': 4307, 'sharon': 4308, 'yeti': 4309, 'karl': 4310, 'rehears': 4311, 'chainsaw': 4312, 'foxx': 4313, 'scoop': 4314, 'fright': 4315, 'paradis': 4316, 'hungri': 4317, 'conscienc': 4318, 'weav': 4319, '1978': 4320, 'thunderbird': 4321, 'suffici': 4322, 'wildli': 4323, 'shortcom': 4324, 'globe': 4325, 'underneath': 4326, 'sung': 4327, '1988': 4328, '1920': 4329, 'prank': 4330, 'tasteless': 4331, 'minu': 4332, 'amazon': 4333, 'stream': 4334, 'lurk': 4335, 'filler': 4336, 'tomorrow': 4337, 'wagner': 4338, 'newcom': 4339, 'hulk': 4340, 'rhythm': 4341, 'hackney': 4342, 'coaster': 4343, 'elit': 4344, '19th': 4345, 'rumor': 4346, 'paramount': 4347, 'el': 4348, 'suspicion': 4349, 'visitor': 4350, 'loneli': 4351, 'aristocrat': 4352, 'spice': 4353, 'basing': 4354, 'financ': 4355, 'choppi': 4356, 'beverli': 4357, 'impos': 4358, 'teas': 4359, 'paxton': 4360, 'abrupt': 4361, 'secondari': 4362, 'rub': 4363, 'grudg': 4364, 'tierney': 4365, 'ingeni': 4366, 'leigh': 4367, 'ram': 4368, 'straightforward': 4369, 'en': 4370, '1989': 4371, 'springer': 4372, 'standout': 4373, 'worship': 4374, 'recogniz': 4375, 'bread': 4376, 'cancer': 4377, '75': 4378, 'immers': 4379, 'heist': 4380, 'literari': 4381, 'counterpart': 4382, 'dirt': 4383, 'brit': 4384, 'inmat': 4385, 'smell': 4386, 'lectur': 4387, 'hopkin': 4388, 'minist': 4389, 'penn': 4390, 'couch': 4391, 'atroc': 4392, 'posey': 4393, 'iv': 4394, 'chamberlain': 4395, 'entranc': 4396, '1939': 4397, 'curli': 4398, 'wwe': 4399, 'naughti': 4400, 'quietli': 4401, 'chavez': 4402, 'watcher': 4403, 'transcend': 4404, 'skeptic': 4405, 'variat': 4406, 'enthral': 4407, 'bernard': 4408, 'duel': 4409, 'heartfelt': 4410, 'policeman': 4411, 'injuri': 4412, 'morbid': 4413, 'nemesi': 4414, 'yearn': 4415, 'esther': 4416, 'geni': 4417, 'entitl': 4418, 'net': 4419, 'clan': 4420, 'missil': 4421, '1986': 4422, 'quaid': 4423, 'ratso': 4424, 'ace': 4425, 'attenborough': 4426, 'lindsay': 4427, 'misguid': 4428, 'sassi': 4429, 'laurenc': 4430, 'sublim': 4431, 'convert': 4432, 'cattl': 4433, 'nolan': 4434, 'moreov': 4435, 'spiral': 4436, 'buzz': 4437, 'graini': 4438, 'characteris': 4439, '1979': 4440, 'mytholog': 4441, 'obstacl': 4442, 'hopelessli': 4443, 'diari': 4444, 'rosemari': 4445, 'grin': 4446, 'facil': 4447, 'poe': 4448, 'tyler': 4449, 'moder': 4450, 'uncut': 4451, 'enabl': 4452, 'carlito': 4453, 'youngest': 4454, 'egg': 4455, 'vader': 4456, 'unexplain': 4457, 'cruelti': 4458, 'setup': 4459, 'kidman': 4460, 'bye': 4461, 'hk': 4462, 'artsi': 4463, 'puppi': 4464, 'dont': 4465, 'brood': 4466, 'reliabl': 4467, 'out': 4468, 'kitti': 4469, 'steadi': 4470, 'bean': 4471, 'bounc': 4472, 'sweat': 4473, 'underworld': 4474, 'heap': 4475, 'preming': 4476, 'oblig': 4477, 'fuel': 4478, 'disastr': 4479, 'effici': 4480, 'decept': 4481, 'spontan': 4482, 'hammi': 4483, 'brendan': 4484, 'gillian': 4485, 'despic': 4486, '1969': 4487, 'bewar': 4488, 'athlet': 4489, 'bronson': 4490, 'martha': 4491, 'acquaint': 4492, 'patricia': 4493, 'clueless': 4494, 'gina': 4495, 'baffl': 4496, 'niec': 4497, 'exterior': 4498, 'narrow': 4499, 'weather': 4500, 'christin': 4501, 'kline': 4502, 'hain': 4503, 'enlist': 4504, 'mayhem': 4505, 'tick': 4506, 'sleepwalk': 4507, 'uh': 4508, 'renaiss': 4509, 'goof': 4510, '73': 4511, 'fontain': 4512, 'trigger': 4513, '19': 4514, 'candl': 4515, 'biker': 4516, 'astound': 4517, 'angst': 4518, 'loath': 4519, 'rome': 4520, 'viewpoint': 4521, 'analyz': 4522, 'injur': 4523, 'outlaw': 4524, 'headach': 4525, 'taboo': 4526, 'preachi': 4527, 'sooner': 4528, 'loi': 4529, 'scar': 4530, 'shatter': 4531, 'housewif': 4532, 'harmless': 4533, 'virtu': 4534, 'insipid': 4535, 'lester': 4536, 'suprem': 4537, 'dilemma': 4538, 'hepburn': 4539, 'mermaid': 4540, 'dandi': 4541, 'circu': 4542, 'contempt': 4543, 'immatur': 4544, 'bent': 4545, 'claustrophob': 4546, 'salt': 4547, 'surgeri': 4548, 'whore': 4549, 'hokey': 4550, 'ebert': 4551, 'hooker': 4552, 'tripl': 4553, 'spade': 4554, 'macho': 4555, 'overlong': 4556, 'stimul': 4557, 'camcord': 4558, 'fluff': 4559, 'zoom': 4560, 'filth': 4561, 'slimi': 4562, 'scorses': 4563, 'amor': 4564, 'foolish': 4565, 'redund': 4566, 'stair': 4567, 'intric': 4568, 'steer': 4569, 'dismal': 4570, 'idol': 4571, 'oldest': 4572, 'glorifi': 4573, 'hostag': 4574, 'guin': 4575, 'phenomenon': 4576, 'cassavet': 4577, 'gere': 4578, 'boston': 4579, 'sox': 4580, 'dish': 4581, 'corbett': 4582, 'ariel': 4583, 'nolt': 4584, 'fascist': 4585, 'keen': 4586, 'cush': 4587, 'preced': 4588, 'margin': 4589, 'harold': 4590, 'assert': 4591, 'widescreen': 4592, 'shred': 4593, 'dwarf': 4594, 'flashi': 4595, 'shield': 4596, 'rhyme': 4597, 'cow': 4598, 'perman': 4599, 'schlock': 4600, 'conquer': 4601, 'astronaut': 4602, 'down': 4603, 'spinal': 4604, 'muscl': 4605, 'faint': 4606, 'beard': 4607, 'joker': 4608, 'mutual': 4609, 'messi': 4610, 'antagonist': 4611, 'transplant': 4612, '1981': 4613, 'mount': 4614, 'gabl': 4615, 'gasp': 4616, 'naschi': 4617, 'remad': 4618, 'radiat': 4619, 'corman': 4620, '1976': 4621, 'flirt': 4622, 'trivia': 4623, 'strongest': 4624, 'spree': 4625, 'flag': 4626, 'proport': 4627, 'zane': 4628, 'obligatori': 4629, 'alvin': 4630, 'cohen': 4631, 'frantic': 4632, 'off': 4633, 'neurot': 4634, 'bachelor': 4635, 'ritchi': 4636, 'hara': 4637, 'discern': 4638, 'mobil': 4639, 'departur': 4640, '95': 4641, 'someday': 4642, 'vaniti': 4643, 'interestingli': 4644, 'instruct': 4645, 'carey': 4646, 'flock': 4647, 'info': 4648, 'raj': 4649, '28': 4650, 'boob': 4651, 'brush': 4652, 'inflict': 4653, 'strive': 4654, 'deer': 4655, 'danish': 4656, 'sensual': 4657, 'fishburn': 4658, 'resum': 4659, 'claud': 4660, 'archiv': 4661, 'www': 4662, 'repris': 4663, 'aborigin': 4664, 'divin': 4665, 'persuad': 4666, 'barn': 4667, 'triangl': 4668, 'wield': 4669, 'bitten': 4670, 'scandal': 4671, '1945': 4672, 'mol': 4673, 'senior': 4674, 'dame': 4675, 'dylan': 4676, 'axe': 4677, 'loretta': 4678, 'luka': 4679, 'pixar': 4680, 'frontier': 4681, 'banter': 4682, 'bate': 4683, 'vibrant': 4684, 'harrison': 4685, 'biblic': 4686, 'artwork': 4687, 'fragil': 4688, 'miracul': 4689, 'hilar': 4690, 'pacif': 4691, 'traffic': 4692, 'pickford': 4693, 'rot': 4694, 'heartwarm': 4695, 'earnest': 4696, 'hapless': 4697, 'timberlak': 4698, 'ish': 4699, 'carlo': 4700, 'hug': 4701, 'undermin': 4702, 'prophet': 4703, 'proclaim': 4704, 'anton': 4705, 'mobster': 4706, 'kathryn': 4707, 'cher': 4708, 'jade': 4709, 'casino': 4710, 'europa': 4711, 'cliffhang': 4712, 'cb': 4713, 'colin': 4714, 'submit': 4715, 'clad': 4716, 'melissa': 4717, 'wendigo': 4718, 'parson': 4719, 'recit': 4720, 'dim': 4721, 'cycl': 4722, 'neill': 4723, 'helm': 4724, 'toronto': 4725, 'akin': 4726, 'token': 4727, 'wardrob': 4728, 'winchest': 4729, 'bikini': 4730, 'lucil': 4731, 'electron': 4732, 'antholog': 4733, 'aris': 4734, 'feast': 4735, 'pc': 4736, 'static': 4737, 'sicken': 4738, 'redneck': 4739, 'uma': 4740, 'mason': 4741, 'legitim': 4742, 'jordan': 4743, 'venom': 4744, 'seedi': 4745, 'shepherd': 4746, 'misfortun': 4747, 'choke': 4748, 'illus': 4749, 'northern': 4750, 'razor': 4751, 'bondag': 4752, 'estrang': 4753, 'breakfast': 4754, 'lui': 4755, 'trier': 4756, 'http': 4757, 'rooki': 4758, 'cerebr': 4759, 'vile': 4760, 'holocaust': 4761, 'jo': 4762, 'foil': 4763, 'eli': 4764, 'isra': 4765, 'marlon': 4766, 'mathieu': 4767, 'flavor': 4768, 'alexandr': 4769, 'blatantli': 4770, 'nope': 4771, 'articl': 4772, 'orphan': 4773, 'vanessa': 4774, 'milo': 4775, 'highway': 4776, 'magician': 4777, 'leather': 4778, 'nightclub': 4779, 'retriev': 4780, 'psych': 4781, 'comprehend': 4782, 'linear': 4783, 'huston': 4784, 'oppon': 4785, 'frog': 4786, 'gilbert': 4787, 'cartoonish': 4788, 'outdat': 4789, 'turd': 4790, 'feminin': 4791, 'charlton': 4792, 'smack': 4793, 'deem': 4794, 'styliz': 4795, 'knightley': 4796, 'peer': 4797, 'disregard': 4798, 'howl': 4799, 'clinic': 4800, 'shorter': 4801, 'ideolog': 4802, 'swept': 4803, 'boyer': 4804, 'tack': 4805, 'affleck': 4806, 'fifth': 4807, 'gunga': 4808, 'dudley': 4809, 'ceremoni': 4810, 'glare': 4811, 'abund': 4812, 'audrey': 4813, 'wrestler': 4814, 'monument': 4815, 'snatch': 4816, 'senat': 4817, 'chip': 4818, 'collector': 4819, 'uniformli': 4820, 'phenomen': 4821, 'summar': 4822, 'cemeteri': 4823, 'btw': 4824, 'lifeless': 4825, 'goldsworthi': 4826, 'bogu': 4827, '1991': 4828, 'compris': 4829, 'lighter': 4830, 'potter': 4831, 'greet': 4832, 'deliver': 4833, 'newer': 4834, 'evolut': 4835, 'cuban': 4836, 'conrad': 4837, 'whack': 4838, 'boo': 4839, 'lavish': 4840, 'durat': 4841, 'energet': 4842, 'einstein': 4843, 'toe': 4844, '1994': 4845, 'sleaz': 4846, 'spawn': 4847, 'breakdown': 4848, 'salman': 4849, 'tara': 4850, 'spine': 4851, 'plate': 4852, 'corn': 4853, 'moe': 4854, 'mitch': 4855, 'client': 4856, 'bastard': 4857, '4th': 4858, 'braveheart': 4859, '1974': 4860, 'bulk': 4861, 'randolph': 4862, 'signal': 4863, 'jedi': 4864, 'judd': 4865, 'jam': 4866, 'healthi': 4867, 'jare': 4868, 'luxuri': 4869, 'spectacl': 4870, 'ie': 4871, 'bori': 4872, 'liu': 4873, 'alleg': 4874, 'embark': 4875, 'pronounc': 4876, 'clara': 4877, 'lex': 4878, 'firmli': 4879, 'wtf': 4880, 'armstrong': 4881, 'appl': 4882, 'jule': 4883, 'kazan': 4884, 'replay': 4885, 'undead': 4886, 'constitut': 4887, 'gilliam': 4888, 'creek': 4889, 'eleven': 4890, 'inaccuraci': 4891, 'kent': 4892, 'trauma': 4893, 'outright': 4894, 'neatli': 4895, 'historian': 4896, 'belli': 4897, 'cecil': 4898, 'ol': 4899, 'nina': 4900, '1977': 4901, 'undertak': 4902, 'fluid': 4903, 'sorrow': 4904, 'capot': 4905, 'occup': 4906, 'mcqueen': 4907, 'evelyn': 4908, 'congratul': 4909, 'cape': 4910, 'bait': 4911, 'truman': 4912, 'abound': 4913, 'pioneer': 4914, 'kiddi': 4915, 'goldblum': 4916, '1985': 4917, 'miami': 4918, 'comprehens': 4919, 'galaxi': 4920, 'curtain': 4921, 'sidewalk': 4922, 'rosario': 4923, 'vain': 4924, 'poker': 4925, 'basket': 4926, 'unsuspect': 4927, 'meal': 4928, 'forgiven': 4929, 'paula': 4930, 'blur': 4931, 'genet': 4932, 'comb': 4933, 'aussi': 4934, 'ash': 4935, 'unattract': 4936, 'fruit': 4937, 'id': 4938, 'conan': 4939, 'groan': 4940, 'mum': 4941, 'carmen': 4942, 'sacrif': 4943, 'vignett': 4944, 'miniseri': 4945, 'pepper': 4946, 'inaccur': 4947, 'inclus': 4948, 'knee': 4949, 'antonio': 4950, 'bsg': 4951, 'relentless': 4952, 'decapit': 4953, 'lauren': 4954, 'walt': 4955, 'porter': 4956, 'propheci': 4957, 'tokyo': 4958, 'roar': 4959, 'palm': 4960, 'spray': 4961, 'lanc': 4962, 'subtli': 4963, 'rapidli': 4964, 'sparkl': 4965, 'modest': 4966, 'incorrect': 4967, 'frontal': 4968, 'dubiou': 4969, 'victorian': 4970, 'hackman': 4971, 'weaker': 4972, 'ingrid': 4973, 'macabr': 4974, 'sophi': 4975, 'cypher': 4976, 'growth': 4977, 'asset': 4978, 'profil': 4979, 'drone': 4980, 'monti': 4981, 'reincarn': 4982, 'ghetto': 4983, 'spill': 4984, 'evan': 4985, 'detach': 4986, 'orchestr': 4987, 'substanti': 4988, 'weari': 4989, 'motorcycl': 4990, 'verg': 4991, 'playboy': 4992, 'scariest': 4993, 'assort': 4994, 'mice': 4995, '21st': 4996, 'bach': 4997, 'scarfac': 4998, 'handicap': 4999}\n"
     ]
    }
   ],
   "source": [
    "word_dict = build_dict(train_X)\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the five most frequently appearing words?\n",
    "\n",
    "<b>Answer:</b> 'movi', 'film', 'one', 'like', 'time'\n",
    "\n",
    "\"movi\" word is nearly 51695 times\n",
    "\n",
    "it doesn't make sence in training because 'movi', 'film', 'one', 'time' has <b>no sentiment.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'film', 'one', 'like', 'time']\n",
      "51695\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use this space to determine the five most frequently appearing words in the training set.\n",
    "word_counts = Counter(np.concatenate( train_X, axis=0 ))\n",
    "sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(sorted_words[0:5])\n",
    "print(word_counts['movi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[42]\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(len(train_X[0]))\n",
    "print(train_X_len[0:1]) # 500 - 333 (non zeros) = 167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Understanding preprocess_data and convert_and_pad_data\n",
    "\n",
    "Answer:\n",
    "\n",
    "* <b>preprocess_data</b> function will cut the data preparation cost, since it is one time process and the result are saved in a file. In case if the notebook instance terminal connection is broken then we need to restart from begining in that case we can skip the data preparation step and proceed to training by directly loading the preprocessed data from file\n",
    "* <b>convert_and_pad_data</b> this function helps to maintain a constant data length. since we using LSTM Clasifier with Embedding so we need to maintain a constant batch size throughout the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the training method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.word_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            output = model(batch_X)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6929407000541687\n",
      "Epoch: 2, BCELoss: 0.6789801836013794\n",
      "Epoch: 3, BCELoss: 0.6659158825874328\n",
      "Epoch: 4, BCELoss: 0.6480816960334778\n",
      "Epoch: 5, BCELoss: 0.6173786520957947\n",
      "Epoch: 6, BCELoss: 0.5791762709617615\n",
      "Epoch: 7, BCELoss: 0.5133359491825104\n",
      "Epoch: 8, BCELoss: 0.4658102333545685\n",
      "Epoch: 9, BCELoss: 0.39789999127388\n",
      "Epoch: 10, BCELoss: 0.3329601466655731\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "#from train.model import LSTMClassifier\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 200, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 10, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads the already completed training job to the Estimater\n",
    "# For first please comment it\n",
    "\n",
    "# my_training_job_name = 'sagemaker-pytorch-2019-02-19-16-44-24-823'\n",
    "# estimator = PyTorch.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 19:28:05 Starting - Starting the training job...\n",
      "2019-08-06 19:28:06 Starting - Launching requested ML instances......\n",
      "2019-08-06 19:29:08 Starting - Preparing the instances for training...\n",
      "2019-08-06 19:30:05 Downloading - Downloading input data...\n",
      "2019-08-06 19:30:30 Training - Downloading the training image...\n",
      "2019-08-06 19:30:48 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:49,209 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:49,212 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:49,225 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:52,252 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:52,477 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:52,477 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:52,477 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-08-06 19:30:52,477 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[31mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[31m  Downloading https://files.pythonhosted.org/packages/69/25/eef8d362bd216b11e7d005331a3cca3d19b0aa57569bde680070109b745c/numpy-1.17.0-cp35-cp35m-manylinux1_x86_64.whl (20.2MB)\u001b[0m\n",
      "\u001b[31mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip (1.5MB)\u001b[0m\n",
      "\u001b[31mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\u001b[0m\n",
      "\u001b[31mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
      "\u001b[31mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl (508kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (1.11.0)\u001b[0m\n",
      "\u001b[31mCollecting soupsieve>=1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/35/e3/25079e8911085ab76a6f2facae0771078260c930216ab0b0c44dc5c9bf31/soupsieve-1.9.2-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[31mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: nltk, train\n",
      "  Running setup.py bdist_wheel for nltk: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/c8/31/48ace4468e236e0e8435f30d33e43df48594e4d53e367cf061\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fr8_hpjd/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built nltk train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pytz, numpy, pandas, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[31m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[31mSuccessfully installed beautifulsoup4-4.8.0 html5lib-1.0.1 nltk-3.4.4 numpy-1.17.0 pandas-0.24.2 pytz-2019.2 soupsieve-1.9.2 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-08-06 19:31:04,283 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-06 19:31:04,297 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"module_name\": \"train\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-08-06-19-28-05-265\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"log_level\": 20,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"num_cpus\": 4,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-952007629372/sagemaker-pytorch-2019-08-06-19-28-05-265/source/sourcedir.tar.gz\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    }\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-952007629372/sagemaker-pytorch-2019-08-06-19-28-05-265/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2019-08-06-19-28-05-265\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-952007629372/sagemaker-pytorch-2019-08-06-19-28-05-265/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 1, BCELoss: 0.6740777468194767\u001b[0m\n",
      "\u001b[31mEpoch: 2, BCELoss: 0.631865820106195\u001b[0m\n",
      "\u001b[31mEpoch: 3, BCELoss: 0.5317807757124609\u001b[0m\n",
      "\u001b[31mEpoch: 4, BCELoss: 0.4539955386093685\u001b[0m\n",
      "\u001b[31mEpoch: 5, BCELoss: 0.3916754898976307\u001b[0m\n",
      "\u001b[31mEpoch: 6, BCELoss: 0.3532942174648752\u001b[0m\n",
      "\u001b[31mEpoch: 7, BCELoss: 0.3393844743164218\u001b[0m\n",
      "\u001b[31mEpoch: 8, BCELoss: 0.3043571880885533\u001b[0m\n",
      "\u001b[31mEpoch: 9, BCELoss: 0.3032935967250746\u001b[0m\n",
      "\u001b[31mEpoch: 10, BCELoss: 0.28959563617803613\u001b[0m\n",
      "\u001b[31m2019-08-06 21:15:43,071 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-08-06 21:15:51 Uploading - Uploading generated training model\n",
      "2019-08-06 21:15:51 Completed - Training job completed\n",
      "Billable seconds: 6346\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Testing the model\n",
    "\n",
    "### Step 6: Deploy the model for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# TODO: Deploy the trained model\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker-pytorch-2019-08-06-19-28-05-265\n",
    "# this function loads the already completed training job to the Estimater\n",
    "# For first please comment it\n",
    "\n",
    "# my_training_job_name = 'sagemaker-pytorch-2019-08-06-19-28-05-265'\n",
    "# estimator = PyTorch.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Use the model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85292"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How does this model compare to the XGBoost model?\n",
    "\n",
    "<b>Answer:</b> XGBoost is an gradient boosting library designed to handle smaller variable problems with lesser training and more accuracy. But Deep learning is designed to large variable data supervised/un-supervised learning but it need more training For sentiment analysis XGBoost is better because XGBoost is the state of the art in most regression and classification problems with better result and less training. since it is mono label classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) More testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8    1    1 ...    0    0    0]\n",
      " [   7 1440 1222 ...    0    0    0]\n",
      " [   4 1222    1 ...    0    0    0]\n",
      " ...\n",
      " [   3 1807    1 ...    0    0    0]\n",
      " [   7 1097 1496 ...    0    0    0]\n",
      " [   5  894 1767 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "data_X = None\n",
    "data_len = None\n",
    "input_data_words = review_to_words(test_review)\n",
    "data_X, data_len = convert_and_pad_data(word_dict, input_data_words)\n",
    "# print(data_len)\n",
    "# print(data_X[0])\n",
    "data_pack = np.column_stack((data_len, data_X))\n",
    "print(data_pack)\n",
    "data_pack = data_pack.reshape(1, -1)\n",
    "\n",
    "data = torch.from_numpy(data_pack)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     0     1     2     3     4     5     6     7    8    ...  490  491  \\\n",
      "0     8     1     1     1  1440  1222   704     1     1    0  ...    0    0   \n",
      "1     7  1440  1222   704     1     1  1097  1206     0    0  ...    0    0   \n",
      "2     4  1222     1  1046   704     0     0     0     0    0  ...    0    0   \n",
      "3     4   425   704     1     1     0     0     0     0    0  ...    0    0   \n",
      "4     4  1046     1  1222     1     0     0     0     0    0  ...    0    0   \n",
      "5     3     1  1496   704     0     0     0     0     0    0  ...    0    0   \n",
      "6     6   894     1     1   425     1  1496     0     0    0  ...    0    0   \n",
      "7     6  1206     1     1  1767   704  1206     0     0    0  ...    0    0   \n",
      "8     5   425     1     1     1   894     0     0     0    0  ...    0    0   \n",
      "9     8     1     1     1  1206     1  1222     1  1496    0  ...    0    0   \n",
      "10    4  1222     1  1730   704     0     0     0     0    0  ...    0    0   \n",
      "11    8     1     1  1730   704  1496     1  1097  1206    0  ...    0    0   \n",
      "12    4     1     1  1730     1     0     0     0     0    0  ...    0    0   \n",
      "13    9     1  1206     1  1496     1   894   704  1496    1  ...    0    0   \n",
      "14    5  1097     1  1097     1  1222     0     0     0    0  ...    0    0   \n",
      "15    7  1807   704   704  1914   704  1496     1     0    0  ...    0    0   \n",
      "16    4  1046     1     1  1206     0     0     0     0    0  ...    0    0   \n",
      "17    3  1807     1     1     0     0     0     0     0    0  ...    0    0   \n",
      "18    7  1097  1496     1     1     1     1  1120     0    0  ...    0    0   \n",
      "19    5   894  1767     1  1206     1     0     0     0    0  ...    0    0   \n",
      "\n",
      "    492  493  494  495  496  497  498  499  \n",
      "0     0    0    0    0    0    0    0    0  \n",
      "1     0    0    0    0    0    0    0    0  \n",
      "2     0    0    0    0    0    0    0    0  \n",
      "3     0    0    0    0    0    0    0    0  \n",
      "4     0    0    0    0    0    0    0    0  \n",
      "5     0    0    0    0    0    0    0    0  \n",
      "6     0    0    0    0    0    0    0    0  \n",
      "7     0    0    0    0    0    0    0    0  \n",
      "8     0    0    0    0    0    0    0    0  \n",
      "9     0    0    0    0    0    0    0    0  \n",
      "10    0    0    0    0    0    0    0    0  \n",
      "11    0    0    0    0    0    0    0    0  \n",
      "12    0    0    0    0    0    0    0    0  \n",
      "13    0    0    0    0    0    0    0    0  \n",
      "14    0    0    0    0    0    0    0    0  \n",
      "15    0    0    0    0    0    0    0    0  \n",
      "16    0    0    0    0    0    0    0    0  \n",
      "17    0    0    0    0    0    0    0    0  \n",
      "18    0    0    0    0    0    0    0    0  \n",
      "19    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[20 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert test_review into a form usable by the model and save the results in test_data\n",
    "test_data = None\n",
    "predictions = None\n",
    "test_review_words = review_to_words(test_review)\n",
    "test_data, test_data_len = convert_and_pad_data(word_dict, test_review_words)\n",
    "test_data = pd.concat([pd.DataFrame(test_data_len), pd.DataFrame(test_data)], axis=1)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48007214, 0.3994868 , 0.2166497 , 0.54820126, 0.42972612,\n",
       "       0.3945137 , 0.6157694 , 0.32787687, 0.53193456, 0.58629394,\n",
       "       0.36237544, 0.5513351 , 0.5443086 , 0.6112492 , 0.6231468 ,\n",
       "       0.47983134, 0.32519004, 0.52465135, 0.67716765, 0.5257694 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model for the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                result_s = predictor.predict(review_input).decode('UTF-8')\n",
    "                result_s = int(float(result_s))\n",
    "                # review_input = int.from_bytes(review_input, byteorder='big', signed=True)\n",
    "                # print(result_s)\n",
    "                results.append(result_s)\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tensor(0.55)\n",
    "result = np.round(out.item(),0)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'\n",
    "predictor.predict(test_review).decode('UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = 'sagemaker-pytorch-2019-02-20-17-49-05-967',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    result = int(float(result))\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up API Gateway\n",
    "### Deploying our web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
